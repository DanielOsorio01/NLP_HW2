{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdRxw48iK5pk"
      },
      "source": [
        "# **Procesamiento de Lenguaje Natural**\n",
        "\n",
        "## **Tarea 2 - Notebook 1**\n",
        "\n",
        "## Integrantes\n",
        "\n",
        "* ### Daniel Osorio Cárdenas\n",
        "* ### Juan Diego Calixto Núñez\n",
        "\n",
        "Este Notebook incluye los literales I, II, III, IV de la tarea. Estos corresponden a la construcción de los modelos de lenguaje que se usarán en el Notebook 2.\n",
        "\n",
        "## **I. Creación de los archivos consolidados**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7Z5cokMaKNPL"
      },
      "outputs": [],
      "source": [
        "# Configurar los directorios donde se encuentran los archivos de datos\n",
        "NEWS_FOLDER = '20news-18828'\n",
        "BLOGS_FOLDER = 'blogs'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Empezamos cargando los archivos de 20news-18828."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tamaño del archivo .txt de 20news-18828:  31843073 bytes =  30.37 MB\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Listar los folders de 20news-18828\n",
        "news_folder = os.listdir(NEWS_FOLDER)\n",
        "\n",
        "# Recorremos cada archivo de news\n",
        "news_texts = []\n",
        "for folder in news_folder:\n",
        "    for file_name in os.listdir(NEWS_FOLDER + '/' + folder):\n",
        "        extracted_text = ''\n",
        "        with open(NEWS_FOLDER + '/' + folder + '/' + file_name, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                # Agregamos solo las lineas del contenido del archivo\n",
        "                if not line.startswith('From:') and not line.startswith('Subject:'):\n",
        "                    extracted_text += line + \" \"\n",
        "            news_texts.append(extracted_text + \"<NEW_DOCUMENT>\")\n",
        "\n",
        "# Guardamos todos los textos en un unico .txt\n",
        "with open('news_consolidated.txt', 'w', encoding='utf-8') as file:\n",
        "    for text in news_texts:\n",
        "        file.write(text)\n",
        "\n",
        "# Revisamos el tamaño del archivo .txt\n",
        "print('Tamaño del archivo .txt de 20news-18828: ', os.path.getsize('news_consolidated.txt'), 'bytes = ', round(os.path.getsize('news_consolidated.txt') / 1048576, 2), 'MB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para cargar los archivos de blogs se hizo primero un preprocesamiento de los archivos XML puesto que hubo muchos errores a la hora de leerlos. La estrategia fue la siguiente:\n",
        "* Se eliminaron los tags de formato del XML\n",
        "* Se eliminaron las fechas de los posts\n",
        "* Se dejó unicamente el contenido de los posts\n",
        "\n",
        "Esto para poder leer los archivos como texto plano. Se aprovecha para saber qué palabras tienen frecuencia 1 por cada archivo para modelar el TKN \\<UNK>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tamaño del archivo .txt de blogs:  758342111 bytes =  723.21 MB\n"
          ]
        }
      ],
      "source": [
        "# Recorremos cada archivo de blogs\n",
        "blogs_texts = []\n",
        "for filename in os.listdir(BLOGS_FOLDER):\n",
        "    extracted_text = ''\n",
        "    with open(BLOGS_FOLDER + '/' + filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            extracted_text += line + \" \"\n",
        "        blogs_texts.append(extracted_text + \"<NEW_DOCUMENT>\")\n",
        "\n",
        "# Guardamos todos los textos en un unico .txt\n",
        "with open('blogs_consolidated.txt', 'w', encoding='utf-8') as file:\n",
        "    for text in blogs_texts:\n",
        "        file.write(text)\n",
        "\n",
        "# Revisamos el tamaño del archivo .txt\n",
        "print('Tamaño del archivo .txt de blogs: ', os.path.getsize('blogs_consolidated.txt'), 'bytes = ', round(os.path.getsize('blogs_consolidated.txt') / 1048576, 2), 'MB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **II y III. Crear archivos de Training y Test y Tokenizar por sentencia**\n",
        "\n",
        "Primero se crean los archivos de training y test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de oraciones en 20news-18828:  469786\n",
            "Cantidad de oraciones en blogs:  13065727\n",
            "Tamaño del archivo .txt de 20N_GROUP_training:  25868385 bytes =  24.67 MB\n",
            "Tamaño del archivo .txt de 20N_GROUP_testing:  6444475 bytes =  6.15 MB\n",
            "Tamaño del archivo .txt de BAC_GROUP_training:  625571254 bytes =  596.59 MB\n",
            "Tamaño del archivo .txt de BAC_GROUP_testing:  145836585 bytes =  139.08 MB\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Cargamos los textos de los archivos .txt\n",
        "with open('news_consolidated.txt', 'r', encoding='utf-8', errors='ignore') as file:\n",
        "    news_texts = file.read()\n",
        "\n",
        "with open('blogs_consolidated.txt', 'r', encoding='utf-8', errors='ignore') as file:\n",
        "    blogs_texts = file.read()\n",
        "\n",
        "# Separar cada archivo en oraciones\n",
        "news_texts = news_texts.split('.')\n",
        "blogs_texts = blogs_texts.split('.')\n",
        "print('Cantidad de oraciones en 20news-18828: ', len(news_texts))\n",
        "print('Cantidad de oraciones en blogs: ', len(blogs_texts))\n",
        "\n",
        "# Dividir los textos en 80% train y 20% test\n",
        "news_texts_train = news_texts[:int(len(news_texts) * 0.8)]\n",
        "news_texts_test = news_texts[int(len(news_texts) * 0.8):]\n",
        "blogs_texts_train = blogs_texts[:int(len(blogs_texts) * 0.8)]\n",
        "blogs_texts_test = blogs_texts[int(len(blogs_texts) * 0.8):]\n",
        "\n",
        "# Guardar los textos de train y test en archivos .txt\n",
        "with open('20N_GROUP_training.txt', 'w', encoding='utf-8') as file:\n",
        "    for text in news_texts_train:\n",
        "        file.write(text + '. ')\n",
        "\n",
        "with open('20N_GROUP_testing.txt', 'w', encoding='utf-8') as file:\n",
        "    for text in news_texts_test:\n",
        "        file.write(text + '. ')\n",
        "\n",
        "with open('BAC_GROUP_training.txt', 'w', encoding='utf-8') as file:\n",
        "    for text in blogs_texts_train:\n",
        "        file.write(text + '. ')\n",
        "\n",
        "with open('BAC_GROUP_testing.txt', 'w', encoding='utf-8') as file:\n",
        "    for text in blogs_texts_test:\n",
        "        file.write(text + '. ')\n",
        "\n",
        "# Revisamos el tamaño de los archivos .txt\n",
        "print('Tamaño del archivo .txt de 20N_GROUP_training: ', os.path.getsize('20N_GROUP_training.txt'), 'bytes = ', round(os.path.getsize('20N_GROUP_training.txt') / 1048576, 2), 'MB')\n",
        "print('Tamaño del archivo .txt de 20N_GROUP_testing: ', os.path.getsize('20N_GROUP_testing.txt'), 'bytes = ', round(os.path.getsize('20N_GROUP_testing.txt') / 1048576, 2), 'MB')\n",
        "print('Tamaño del archivo .txt de BAC_GROUP_training: ', os.path.getsize('BAC_GROUP_training.txt'), 'bytes = ', round(os.path.getsize('BAC_GROUP_training.txt') / 1048576, 2), 'MB')\n",
        "print('Tamaño del archivo .txt de BAC_GROUP_testing: ', os.path.getsize('BAC_GROUP_testing.txt'), 'bytes = ', round(os.path.getsize('BAC_GROUP_testing.txt') / 1048576, 2), 'MB')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Procedemos a cargar los archivos de entrenamiento y a tokenizar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de oraciones en 20news-18828 training:  24090045\n",
            "Cantidad de oraciones en blogs training:  575765517\n"
          ]
        }
      ],
      "source": [
        "# Se cargan los archivos de training\n",
        "with open('20N_GROUP_training.txt', 'r', encoding='utf-8', errors='ignore') as file:\n",
        "    news_texts_train = file.read()\n",
        "\n",
        "with open('BAC_GROUP_training.txt', 'r', encoding='utf-8', errors='ignore') as file:\n",
        "    blogs_texts_train = file.read()\n",
        "\n",
        "# Normalizamos los textos pasando todo a minusculas\n",
        "news_texts_train = news_texts_train.lower()\n",
        "news_texts_train = news_texts_train.lower()\n",
        "\n",
        "# Se separan los textos por \"<NEW_DOCUMENT>\" para obtener los documentos\n",
        "news_texts_train = news_texts_train.split('<NEW_DOCUMENT>')\n",
        "blogs_texts_train = blogs_texts_train.split('<NEW_DOCUMENT>')\n",
        "\n",
        "# Para cada documento, se encuentran los términos con frecuencia 1\n",
        "news_texts = ''\n",
        "for text in news_texts_train:\n",
        "    news_words = {}\n",
        "    for word in text.split():\n",
        "        if word in news_words:\n",
        "            news_words[word] += 1\n",
        "        else:\n",
        "            news_words[word] = 1\n",
        "    extracted_text = ''\n",
        "    extracted_text += ' '.join([word if news_words[word] > 1 else '<UNK>' for word in text.split()])\n",
        "    news_texts += extracted_text\n",
        "\n",
        "blogs_texts = ''\n",
        "for text in blogs_texts_train:\n",
        "    blogs_words = {}\n",
        "    for word in text.split():\n",
        "        if word in blogs_words:\n",
        "            blogs_words[word] += 1\n",
        "        else:\n",
        "            blogs_words[word] = 1\n",
        "    extracted_text = ''\n",
        "    extracted_text += ' '.join([word if blogs_words[word] > 1 else '<UNK>' for word in text.split()])\n",
        "    blogs_texts += extracted_text\n",
        "\n",
        "# Se reemplazan los números por el token <NUM>\n",
        "news_texts= re.sub(r'\\d+', '<NUM>', news_texts)\n",
        "blogs_texts = re.sub(r'\\d+', '<NUM>', blogs_texts)\n",
        "\n",
        "# La marcación de TKN con frecuencia 1 se realizo en el paso anterior\n",
        "# cuando se crearon los archivos consolidados\n",
        "\n",
        "# Se separan los textos por '.' para obtener oraciones\n",
        "news_texts_sentences = news_texts.split('.')\n",
        "news_texts_sentences = blogs_texts.split('.')\n",
        "print('Cantidad de oraciones en 20news-18828 training: ', len(news_texts))\n",
        "print('Cantidad de oraciones en blogs training: ', len(blogs_texts))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
