{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdRxw48iK5pk"
      },
      "source": [
        "# **Procesamiento de Lenguaje Natural**\n",
        "\n",
        "## **Tarea 2 - Notebook 1**\n",
        "\n",
        "## Integrantes\n",
        "\n",
        "* ### Daniel Osorio Cárdenas\n",
        "* ### Juan Diego Calixto Núñez\n",
        "\n",
        "Este Notebook incluye los literales I, II, III, IV de la tarea. Estos corresponden a la construcción de los modelos de lenguaje que se usarán en el Notebook 2.\n",
        "\n",
        "## **I. Creación de los archivos consolidados**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7Z5cokMaKNPL"
      },
      "outputs": [],
      "source": [
        "# Configurar los directorios donde se encuentran los archivos de datos\n",
        "NEWS_FOLDER = '20news-18828'\n",
        "BLOGS_FOLDER = 'blogs'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Empezamos cargando los archivos de 20news-18828."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tamaño del archivo 20N_consolidated.txt:  31832529 bytes =  30.36 MB\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Listar los folders de 20news-18828\n",
        "news_folder = os.listdir(NEWS_FOLDER)\n",
        "\n",
        "# Recorremos cada archivo de news\n",
        "news_texts = []\n",
        "for folder in news_folder:\n",
        "    for file_name in os.listdir(NEWS_FOLDER + '/' + folder):\n",
        "        extracted_text = ''\n",
        "        with open(NEWS_FOLDER + '/' + folder + '/' + file_name, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                # Agregamos solo las lineas del contenido del archivo\n",
        "                if not line.startswith('From:') and not line.startswith('Subject:') and not line.startswith('Fax:') and not line.startswith('Phone:') and not line.startswith('Email:') and not line.startswith('INTERNET:'):\n",
        "                    extracted_text += line + \" \"\n",
        "            news_texts.append(extracted_text + \"<NEW_DOCUMENT>\")\n",
        "\n",
        "# Guardamos todos los textos en un unico .txt\n",
        "with open('20N_consolidated.txt', 'w', encoding='utf-8') as file:\n",
        "    for text in news_texts:\n",
        "        file.write(text)\n",
        "\n",
        "# Revisamos el tamaño del archivo .txt\n",
        "print('Tamaño del archivo 20N_consolidated.txt: ', os.path.getsize('20N_consolidated.txt'), 'bytes = ', round(os.path.getsize('20N_consolidated.txt') / 1048576, 2), 'MB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para cargar los archivos de blogs se hizo primero un preprocesamiento de los archivos XML puesto que hubo muchos errores a la hora de leerlos. La estrategia fue la siguiente:\n",
        "* Se eliminaron los tags de formato del XML\n",
        "* Se eliminaron las fechas de los posts\n",
        "* Se dejó unicamente el contenido de los posts\n",
        "\n",
        "Esto para poder leer los archivos como texto plano. Se aprovecha para saber qué palabras tienen frecuencia 1 por cada archivo para modelar el TKN \\<UNK>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tamaño del archivo BAC_consolidated.txt:  757660374 bytes =  722.56 MB\n"
          ]
        }
      ],
      "source": [
        "# Recorremos cada archivo de blogs\n",
        "blogs_texts = []\n",
        "for filename in os.listdir(BLOGS_FOLDER):\n",
        "    extracted_text = ''\n",
        "    with open(BLOGS_FOLDER + '/' + filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            # Agregamos solo las lineas del contenido del archivo\n",
        "            if not line.startswith('From:') and not line.startswith('Subject:') and not line.startswith('Fax:') and not line.startswith('Phone:') and not line.startswith('Email:') and not line.startswith('INTERNET:'):\n",
        "                extracted_text += line + \" \"\n",
        "        blogs_texts.append(extracted_text + \"<NEW_DOCUMENT>\")\n",
        "\n",
        "# Guardamos todos los textos en un unico .txt\n",
        "with open('BAC_consolidated.txt', 'w', encoding='utf-8') as file:\n",
        "    for text in blogs_texts:\n",
        "        file.write(text)\n",
        "\n",
        "# Revisamos el tamaño del archivo .txt\n",
        "print('Tamaño del archivo BAC_consolidated.txt: ', os.path.getsize('BAC_consolidated.txt'), 'bytes = ', round(os.path.getsize('BAC_consolidated.txt') / 1048576, 2), 'MB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **II y III. Crear archivos de Training y Test y Tokenizar por sentencia**\n",
        "\n",
        "Primero se crean los archivos de training y test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de oraciones en 20news-18828:  425274\n",
            "Cantidad de oraciones en BAC:  9425954\n",
            "Tamaño del archivo .txt de 20N_GROUP_training:  25475664 bytes =  24.3 MB\n",
            "Tamaño del archivo .txt de 20N_GROUP_testing:  6323560 bytes =  6.03 MB\n",
            "Tamaño del archivo .txt de BAC_GROUP_training:  599771948 bytes =  571.99 MB\n",
            "Tamaño del archivo .txt de BAC_GROUP_testing:  148560451 bytes =  141.68 MB\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Cargamos los textos de los archivos .txt\n",
        "with open('20N_consolidated.txt', 'r', encoding='utf-8', errors='ignore') as file:\n",
        "    news_texts = file.read()\n",
        "\n",
        "with open('BAC_consolidated.txt', 'r', encoding='utf-8', errors='ignore') as file:\n",
        "    blogs_texts = file.read()\n",
        "\n",
        "# Separar cada archivo en oraciones\n",
        "news_texts = news_texts.split('.')\n",
        "blogs_texts = blogs_texts.split('.')\n",
        "\n",
        "# Quitar los textos vacios\n",
        "news_texts = [text for text in news_texts if text != '']\n",
        "blogs_texts = [text for text in blogs_texts if text != '']\n",
        "\n",
        "print('Cantidad de oraciones en 20news-18828: ', len(news_texts))\n",
        "print('Cantidad de oraciones en BAC: ', len(blogs_texts))\n",
        "\n",
        "# Dividir los textos en 80% train y 20% test\n",
        "news_texts_train = news_texts[:int(len(news_texts) * 0.8)]\n",
        "news_texts_test = news_texts[int(len(news_texts) * 0.8):]\n",
        "blogs_texts_train = blogs_texts[:int(len(blogs_texts) * 0.8)]\n",
        "blogs_texts_test = blogs_texts[int(len(blogs_texts) * 0.8):]\n",
        "\n",
        "# Guardar los textos de train y test en archivos .txt\n",
        "with open('20N_GROUP_training.txt', 'w', encoding='utf-8') as file:\n",
        "    for text in news_texts_train:\n",
        "        file.write(text.strip() + '. ')\n",
        "\n",
        "with open('20N_GROUP_testing.txt', 'w', encoding='utf-8') as file:\n",
        "    for text in news_texts_test:\n",
        "        file.write(text.strip() + '. ')\n",
        "\n",
        "with open('BAC_GROUP_training.txt', 'w', encoding='utf-8') as file:\n",
        "    for text in blogs_texts_train:\n",
        "        file.write(text.strip() + '. ')\n",
        "\n",
        "with open('BAC_GROUP_testing.txt', 'w', encoding='utf-8') as file:\n",
        "    for text in blogs_texts_test:\n",
        "        file.write(text.strip() + '. ')\n",
        "\n",
        "# Revisamos el tamaño de los archivos .txt\n",
        "print('Tamaño del archivo .txt de 20N_GROUP_training: ', os.path.getsize('20N_GROUP_training.txt'), 'bytes = ', round(os.path.getsize('20N_GROUP_training.txt') / 1048576, 2), 'MB')\n",
        "print('Tamaño del archivo .txt de 20N_GROUP_testing: ', os.path.getsize('20N_GROUP_testing.txt'), 'bytes = ', round(os.path.getsize('20N_GROUP_testing.txt') / 1048576, 2), 'MB')\n",
        "print('Tamaño del archivo .txt de BAC_GROUP_training: ', os.path.getsize('BAC_GROUP_training.txt'), 'bytes = ', round(os.path.getsize('BAC_GROUP_training.txt') / 1048576, 2), 'MB')\n",
        "print('Tamaño del archivo .txt de BAC_GROUP_testing: ', os.path.getsize('BAC_GROUP_testing.txt'), 'bytes = ', round(os.path.getsize('BAC_GROUP_testing.txt') / 1048576, 2), 'MB')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      I've been selling books online since April, and am painfully aware of the vast ocean of book-knowledge that I haven't yet begun to cross\n"
          ]
        }
      ],
      "source": [
        "print(blogs_texts_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Procedemos a cargar los archivos de entrenamiento y a tokenizar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de documentos en 20N_GROUP_training:  14716\n",
            "Cantidad de documentos en BAC_GROUP_training:  15324\n",
            "The M code stream  might be independently attacked based on knowledge of clipper chip protocols as revealed plaintext. This could be invalidated by changing the temporal and or spatial relationship of the clipper M stream and the actual transmitted stream, under the control of a secure key generator synchronized between endpoints. The useful life time of captured law enforcement blocks might be limited based on hostile forces using them as targets following transmission interception. You would need a large number of them, but, hey there's supposed to be millions of these things, right?  Adding time stamps to the encrypted law enforcement block is probably impractical, who wants an encryption chip with a real time clock?  *****************************************************************************  The entire idea of the law enforcement block can be invalidated. I just had the thought, that you could capture your own law enforcement blocks for session keys K that you will not use in actual transmissions as the session key authenticators. The proviso that you don't mind your own serial number being discovered. d. denning just sent out further information of a new version of the clipper chip. If a hash function were to be embedded in a clipper M transmission block reflecting the law enforcement block, it better not fall on 64 bit block boundaries. If it were a recognizeable datum, you could lie with it too. I like the randomizer inclusion in the MYK-80. I remember reading that Intel had an approved random noise source on silicon, hence the ability to put it Inside. You ever think that Mykotronx sounds like one of those made up names of companies used as fronts for intelligence organizations?           \n",
            "['the', 'm', 'code', 'stream', 'might', 'be', 'independently', 'attacked', 'based', 'on', 'knowledge', 'of', 'clipper', 'chip', 'protocols', 'as', 'revealed', 'plaintext.', 'this', 'could', 'be', 'invalidated', 'by', 'changing', 'the', 'temporal', 'and', 'or', 'spatial', 'relationship', 'of', 'the', 'clipper', 'm', 'stream', 'and', 'the', 'actual', 'transmitted', 'stream,', 'under', 'the', 'control', 'of', 'a', 'secure', 'key', 'generator', 'synchronized', 'between', 'endpoints.', 'the', 'useful', 'life', 'time', 'of', 'captured', 'law', 'enforcement', 'blocks', 'might', 'be', 'limited', 'based', 'on', 'hostile', 'forces', 'using', 'them', 'as', 'targets', 'following', 'transmission', 'interception.', 'you', 'would', 'need', 'a', 'large', 'number', 'of', 'them,', 'but,', 'hey', \"there's\", 'supposed', 'to', 'be', 'millions', 'of', 'these', 'things,', 'right?', 'adding', 'time', 'stamps', 'to', 'the', 'encrypted', 'law', 'enforcement', 'block', 'is', 'probably', 'impractical,', 'who', 'wants', 'an', 'encryption', 'chip', 'with', 'a', 'real', 'time', 'clock?', '*****************************************************************************', 'the', 'entire', 'idea', 'of', 'the', 'law', 'enforcement', 'block', 'can', 'be', 'invalidated.', 'i', 'just', 'had', 'the', 'thought,', 'that', 'you', 'could', 'capture', 'your', 'own', 'law', 'enforcement', 'blocks', 'for', 'session', 'keys', 'k', 'that', 'you', 'will', 'not', 'use', 'in', 'actual', 'transmissions', 'as', 'the', 'session', 'key', 'authenticators.', 'the', 'proviso', 'that', 'you', \"don't\", 'mind', 'your', 'own', 'serial', 'number', 'being', 'discovered.', 'd.', 'denning', 'just', 'sent', 'out', 'further', 'information', 'of', 'a', 'new', 'version', 'of', 'the', 'clipper', 'chip.', 'if', 'a', 'hash', 'function', 'were', 'to', 'be', 'embedded', 'in', 'a', 'clipper', 'm', 'transmission', 'block', 'reflecting', 'the', 'law', 'enforcement', 'block,', 'it', 'better', 'not', 'fall', 'on', '64', 'bit', 'block', 'boundaries.', 'if', 'it', 'were', 'a', 'recognizeable', 'datum,', 'you', 'could', 'lie', 'with', 'it', 'too.', 'i', 'like', 'the', 'randomizer', 'inclusion', 'in', 'the', 'myk-80.', 'i', 'remember', 'reading', 'that', 'intel', 'had', 'an', 'approved', 'random', 'noise', 'source', 'on', 'silicon,', 'hence', 'the', 'ability', 'to', 'put', 'it', 'inside.', 'you', 'ever', 'think', 'that', 'mykotronx', 'sounds', 'like', 'one', 'of', 'those', 'made', 'up', 'names', 'of', 'companies', 'used', 'as', 'fronts', 'for', 'intelligence', 'organizations?']\n",
            "<s> the m <UNK> stream might be <UNK> <UNK> based on <UNK> of clipper chip <UNK> as <UNK> plaintext </s> <s> <UNK> could be invalidated <UNK> <UNK> the <UNK> and <UNK> <UNK> <UNK> of the clipper m stream and the actual <UNK> <UNK> <UNK> the <UNK> of a <UNK> key <UNK> <UNK> <UNK> endpoints </s> <s> the <UNK> <UNK> time of <UNK> law enforcement blocks might be <UNK> based on <UNK> <UNK> <UNK> <UNK> as <UNK> <UNK> transmission interception </s> <s> you <UNK> <UNK> a <UNK> number of <UNK> <UNK> <UNK> <UNK> <UNK> to be <UNK> of <UNK> <UNK> <UNK> <UNK> time <UNK> to the <UNK> law enforcement block <UNK> <UNK> <UNK> <UNK> <UNK> an <UNK> chip with a <UNK> time <UNK> <UNK> the <UNK> <UNK> of the law enforcement block <UNK> be invalidated </s> <s> i just had the <UNK> that you could <UNK> your own law enforcement blocks for session <UNK> <UNK> that you <UNK> not <UNK> in actual <UNK> as the session key authenticators </s> <s> the <UNK> that you <UNK> <UNK> your own <UNK> number <UNK> discovered </s> <s> d </s> <s> <UNK> just <UNK> <UNK> <UNK> <UNK> of a <UNK> <UNK> of the clipper chip </s> <s> if a <UNK> <UNK> were to be <UNK> in a clipper m transmission block <UNK> the law enforcement <UNK> it <UNK> not <UNK> on <UNK> <UNK> block boundaries </s> <s> if it were a <UNK> <UNK> you could <UNK> with it too </s> <s> i like the <UNK> <UNK> in the myk-<NUM> </s> <s> i <UNK> <UNK> that <UNK> had an <UNK> <UNK> <UNK> <UNK> on <UNK> <UNK> the <UNK> to <UNK> it inside </s> <s> you <UNK> <UNK> that <UNK> <UNK> like <UNK> of <UNK> <UNK> <UNK> <UNK> of <UNK> <UNK> as <UNK> for <UNK> <UNK> </s>\n"
          ]
        }
      ],
      "source": [
        "# Se cargan los archivos de training\n",
        "with open('20N_GROUP_training.txt', 'r', encoding='utf-8', errors='ignore') as file:\n",
        "    news_texts_train = file.read()\n",
        "\n",
        "with open('BAC_GROUP_training.txt', 'r', encoding='utf-8', errors='ignore') as file:\n",
        "    blogs_texts_train = file.read()\n",
        "\n",
        "# Se separan los textos por \"<NEW_DOCUMENT>\" para obtener los documentos\n",
        "news_texts_train = news_texts_train.split('<NEW_DOCUMENT>')\n",
        "blogs_texts_train = blogs_texts_train.split('<NEW_DOCUMENT>')\n",
        "\n",
        "print('Cantidad de documentos en 20N_GROUP_training: ', len(news_texts_train))\n",
        "print('Cantidad de documentos en BAC_GROUP_training: ', len(blogs_texts_train))\n",
        "\n",
        "# Se recorre cada documento y se encuentran los terminos con frecuencia 1 en cada documento\n",
        "for text in news_texts_train:\n",
        "    print(text)\n",
        "    # Se normaliza el texto\n",
        "    text = text.lower()\n",
        "    # Se reemplazan todos los numeros por <NUM>\n",
        "    text = re.sub(r'\\d+', '<NUM>', text)\n",
        "    # Se crea un diccionario donde guardamos la frecuencia de cada termino en el documento\n",
        "    word_freq = {}\n",
        "    # Se separa el texto en palabras\n",
        "    words = text.split()\n",
        "    print(words)\n",
        "    # Se recorre cada palabra\n",
        "    for word in words:\n",
        "        if word != '<NUM>':\n",
        "            # Si la palabra tiene un . al final, se quita\n",
        "            if word.endswith('.'):\n",
        "                word = word[:-1]\n",
        "            # Se suma 1 a la cuenta de la palabra si existe\n",
        "            if word in word_freq:\n",
        "                word_freq[word] += 1\n",
        "            # Se agrega la palabra al diccionario si no existe\n",
        "            else:\n",
        "                word_freq[word] = 1\n",
        "    # Se recorre cada palabra del diccionario\n",
        "    for word in word_freq:\n",
        "        # Si la palabra tiene frecuencia 1, se reemplaza por <UNK>\n",
        "        if word_freq[word] == 1:\n",
        "            text = text.replace(' ' + word + ' ', ' <UNK> ')\n",
        "    # Se reemplazan todos los . por </s> y <s>\n",
        "    text = text.replace('.', ' </s> <s> ')\n",
        "    # Se reemplazan todos los espacios en blanco por un solo espacio\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Se reemplazan los espacios en blanco al inicio y al final del texto\n",
        "    text = text.strip()\n",
        "    # Se agrega un <s> al inicio y un </s> al final del texto\n",
        "    text = '<s> ' + text + ' </s>'\n",
        "    print(text)\n",
        "    break\n",
        "\n",
        "            \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The M code stream  might be independently attacked based on knowledge of clipper chip protocols as revealed plaintext. This could be invalidated by changing the temporal and or spatial relationship of the clipper M stream and the actual transmitted stream, under the control of a secure key generator synchronized between endpoints. The useful life time of captured law enforcement blocks might be limited based on hostile forces using them as targets following transmission interception. You would need a large number of them, but, hey there's supposed to be millions of these things, right?  Adding time stamps to the encrypted law enforcement block is probably impractical, who wants an encryption chip with a real time clock?  *****************************************************************************  The entire idea of the law enforcement block can be invalidated. I just had the thought, that you could capture your own law enforcement blocks for session keys K that you will not use in actual transmissions as the session key authenticators. The proviso that you don't mind your own serial number being discovered. d. denning just sent out further information of a new version of the clipper chip. If a hash function were to be embedded in a clipper M transmission block reflecting the law enforcement block, it better not fall on 64 bit block boundaries. If it were a recognizeable datum, you could lie with it too. I like the randomizer inclusion in the MYK-80. I remember reading that Intel had an approved random noise source on silicon, hence the ability to put it Inside. You ever think that Mykotronx sounds like one of those made up names of companies used as fronts for intelligence organizations?           \n"
          ]
        }
      ],
      "source": [
        "print(news_texts_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Se cargan los archivos de training\n",
        "with open('20N_GROUP_training.txt', 'r', encoding='utf-8', errors='ignore') as file:\n",
        "    news_texts_train = file.read()\n",
        "\n",
        "with open('BAC_GROUP_training.txt', 'r', encoding='utf-8', errors='ignore') as file:\n",
        "    blogs_texts_train = file.read()\n",
        "\n",
        "# Normalizamos los textos pasando todo a minusculas\n",
        "news_texts_train = news_texts_train.lower()\n",
        "blogs_texts_train = blogs_texts_train.lower()\n",
        "\n",
        "# Se separan los textos por \"<NEW_DOCUMENT>\" para obtener los documentos\n",
        "news_texts_train = news_texts_train.split('<NEW_DOCUMENT>')\n",
        "blogs_texts_train = blogs_texts_train.split('<NEW_DOCUMENT>')\n",
        "\n",
        "# Para cada documento, se encuentran los términos con frecuencia 1\n",
        "news_texts = ''\n",
        "for text in news_texts_train:\n",
        "    news_words = {}\n",
        "    for word in text.split():\n",
        "        if word in news_words:\n",
        "            news_words[word] += 1\n",
        "        else:\n",
        "            news_words[word] = 1\n",
        "    extracted_text = ''\n",
        "    extracted_text += ' '.join([word if news_words[word] > 1 else '<UNK>' for word in text.split()])\n",
        "    news_texts += extracted_text\n",
        "\n",
        "blogs_texts = ''\n",
        "for text in blogs_texts_train:\n",
        "    blogs_words = {}\n",
        "    for word in text.split():\n",
        "        if word in blogs_words:\n",
        "            blogs_words[word] += 1\n",
        "        else:\n",
        "            blogs_words[word] = 1\n",
        "    extracted_text = ''\n",
        "    extracted_text += ' '.join([word if blogs_words[word] > 1 else '<UNK>' for word in text.split()])\n",
        "    blogs_texts += extracted_text\n",
        "\n",
        "# Se reemplazan los números por el token <NUM>\n",
        "news_texts= re.sub(r'\\d+', '<NUM>', news_texts)\n",
        "blogs_texts = re.sub(r'\\d+', '<NUM>', blogs_texts)\n",
        "\n",
        "# La marcación de TKN con frecuencia 1 se realizo en el paso anterior\n",
        "# cuando se crearon los archivos consolidados\n",
        "\n",
        "# Se separan los textos por '.' para obtener oraciones\n",
        "news_texts_sentences = news_texts.split('.')\n",
        "news_texts_sentences = blogs_texts.split('.')\n",
        "print('Cantidad de oraciones en 20news-18828 training: ', len(news_texts))\n",
        "print('Cantidad de oraciones en blogs training: ', len(blogs_texts))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
