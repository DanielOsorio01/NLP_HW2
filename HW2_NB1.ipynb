{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdRxw48iK5pk"
      },
      "source": [
        "# **Procesamiento de Lenguaje Natural**\n",
        "\n",
        "## **Tarea 2 - Notebook 1**\n",
        "\n",
        "## Integrantes\n",
        "\n",
        "* ### Daniel Osorio Cárdenas\n",
        "* ### Juan Diego Calixto Núñez\n",
        "\n",
        "Este Notebook incluye los literales I, II, III, IV de la tarea. Estos corresponden a la construcción de los modelos de lenguaje que se usarán en el Notebook 2.\n",
        "\n",
        "## **I. Creación de los archivos consolidados**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7Z5cokMaKNPL"
      },
      "outputs": [],
      "source": [
        "# Configurar los directorios donde se encuentran los archivos de datos\n",
        "NEWS_FOLDER = '20news-18828'\n",
        "BLOGS_FOLDER = 'blogs'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Empezamos cargando los archivos de 20news-18828 y creamos un único archivo consolidado llamado \"20N_consolidated.txt\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tamaño del archivo 20N_consolidated.txt:  31568937 bytes =  30.11 MB\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Listar los folders de 20news-18828\n",
        "news_folder = os.listdir(NEWS_FOLDER)\n",
        "\n",
        "# Recorremos cada archivo de news\n",
        "news_texts = []\n",
        "for folder in news_folder:\n",
        "    for file_name in os.listdir(NEWS_FOLDER + '/' + folder):\n",
        "        extracted_text = ''\n",
        "        with open(NEWS_FOLDER + '/' + folder + '/' + file_name, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                # Agregamos solo las lineas del contenido del archivo\n",
        "                if not line.startswith('From:') and not line.startswith('Subject:') and not line.startswith('Fax:') and not line.startswith('Phone:') and not line.startswith('Email:') and not line.startswith('INTERNET:'):\n",
        "                    extracted_text += line + \" \"\n",
        "            news_texts.append(extracted_text)\n",
        "\n",
        "# Guardamos todos los textos en un unico .txt\n",
        "with open('20N_consolidated.txt', 'w', encoding='utf-8') as file:\n",
        "    for text in news_texts:\n",
        "        file.write(text)\n",
        "\n",
        "# Revisamos el tamaño del archivo .txt\n",
        "print('Tamaño del archivo 20N_consolidated.txt: ', os.path.getsize('20N_consolidated.txt'), 'bytes = ', round(os.path.getsize('20N_consolidated.txt') / 1048576, 2), 'MB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para cargar los archivos de blogs se hizo primero un preprocesamiento de los archivos XML puesto que hubo muchos errores a la hora de leerlos. La estrategia fue la siguiente:\n",
        "* Se eliminaron los tags de formato del XML\n",
        "* Se eliminaron las fechas de los posts\n",
        "* Se dejó unicamente el contenido de los posts\n",
        "\n",
        "Esto para poder leer los archivos como texto plano. El archivo que hace esta modificación se llama \"blogs_preprocessing.py\".\n",
        "\n",
        "Finalmente, se cargaron los archivos de blogs y se creó un único archivo consolidado llamado \"BAC_consolidated.txt\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tamaño del archivo BAC_consolidated.txt:  757389894 bytes =  722.3 MB\n"
          ]
        }
      ],
      "source": [
        "# Recorremos cada archivo de blogs\n",
        "blogs_texts = []\n",
        "for filename in os.listdir(BLOGS_FOLDER):\n",
        "    extracted_text = ''\n",
        "    with open(BLOGS_FOLDER + '/' + filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            # Agregamos solo las lineas del contenido del archivo\n",
        "            if not line.startswith('From:') and not line.startswith('Subject:') and not line.startswith('Fax:') and not line.startswith('Phone:') and not line.startswith('Email:') and not line.startswith('INTERNET:'):\n",
        "                extracted_text += line + \" \"\n",
        "        blogs_texts.append(extracted_text)\n",
        "\n",
        "# Guardamos todos los textos en un unico .txt\n",
        "with open('BAC_consolidated.txt', 'w', encoding='utf-8') as file:\n",
        "    for text in blogs_texts:\n",
        "        file.write(text)\n",
        "\n",
        "# Revisamos el tamaño del archivo .txt\n",
        "print('Tamaño del archivo BAC_consolidated.txt: ', os.path.getsize('BAC_consolidated.txt'), 'bytes = ', round(os.path.getsize('BAC_consolidated.txt') / 1048576, 2), 'MB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **II y III. Crear archivos de Training y Test y Tokenizar por sentencia**\n",
        "\n",
        "Primero se crean los archivos de training y test. Para esto se separó cada archivo consolidado por oraciones. Suponemos que las oraciones están separadas por puntos. Se crearon dos archivos por cada corpus, uno de training y otro de test. Los archivos de training contienen el 80% de las oraciones y los de test el 20% restante."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de oraciones en 20news-18828:  425274\n",
            "Cantidad de oraciones en BAC:  9425954\n",
            "Tamaño del archivo .txt de 20N_GROUP_training:  25266186 bytes =  24.1 MB\n",
            "Tamaño del archivo .txt de 20N_GROUP_testing:  6265196 bytes =  5.97 MB\n",
            "Tamaño del archivo .txt de BAC_GROUP_training:  599507899 bytes =  571.74 MB\n",
            "Tamaño del archivo .txt de BAC_GROUP_testing:  148491073 bytes =  141.61 MB\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Cargamos los textos de los archivos .txt\n",
        "with open('20N_consolidated.txt', 'r', encoding='utf-8', errors='ignore') as file:\n",
        "    news_texts = file.read()\n",
        "\n",
        "with open('BAC_consolidated.txt', 'r', encoding='utf-8', errors='ignore') as file:\n",
        "    blogs_texts = file.read()\n",
        "\n",
        "# Separar cada archivo en oraciones\n",
        "news_texts = news_texts.split('.')\n",
        "blogs_texts = blogs_texts.split('.')\n",
        "\n",
        "# Quitar los textos vacios\n",
        "news_texts = [text for text in news_texts if text != '']\n",
        "blogs_texts = [text for text in blogs_texts if text != '']\n",
        "\n",
        "print('Cantidad de oraciones en 20news-18828: ', len(news_texts))\n",
        "print('Cantidad de oraciones en BAC: ', len(blogs_texts))\n",
        "\n",
        "# Dividir los textos en 80% train y 20% test\n",
        "news_texts_train = news_texts[:int(len(news_texts) * 0.8)]\n",
        "news_texts_test = news_texts[int(len(news_texts) * 0.8):]\n",
        "blogs_texts_train = blogs_texts[:int(len(blogs_texts) * 0.8)]\n",
        "blogs_texts_test = blogs_texts[int(len(blogs_texts) * 0.8):]\n",
        "\n",
        "# Guardar los textos de train y test en archivos .txt\n",
        "with open('20N_GROUP_training.txt', 'w', encoding='utf-8') as file:\n",
        "    for text in news_texts_train:\n",
        "        file.write(text.strip() + '. ')\n",
        "\n",
        "with open('20N_GROUP_testing.txt', 'w', encoding='utf-8') as file:\n",
        "    for text in news_texts_test:\n",
        "        file.write(text.strip() + '. ')\n",
        "\n",
        "with open('BAC_GROUP_training.txt', 'w', encoding='utf-8') as file:\n",
        "    for text in blogs_texts_train:\n",
        "        file.write(text.strip() + '. ')\n",
        "\n",
        "with open('BAC_GROUP_testing.txt', 'w', encoding='utf-8') as file:\n",
        "    for text in blogs_texts_test:\n",
        "        file.write(text.strip() + '. ')\n",
        "\n",
        "# Revisamos el tamaño de los archivos .txt\n",
        "print('Tamaño del archivo .txt de 20N_GROUP_training: ', os.path.getsize('20N_GROUP_training.txt'), 'bytes = ', round(os.path.getsize('20N_GROUP_training.txt') / 1048576, 2), 'MB')\n",
        "print('Tamaño del archivo .txt de 20N_GROUP_testing: ', os.path.getsize('20N_GROUP_testing.txt'), 'bytes = ', round(os.path.getsize('20N_GROUP_testing.txt') / 1048576, 2), 'MB')\n",
        "print('Tamaño del archivo .txt de BAC_GROUP_training: ', os.path.getsize('BAC_GROUP_training.txt'), 'bytes = ', round(os.path.getsize('BAC_GROUP_training.txt') / 1048576, 2), 'MB')\n",
        "print('Tamaño del archivo .txt de BAC_GROUP_testing: ', os.path.getsize('BAC_GROUP_testing.txt'), 'bytes = ', round(os.path.getsize('BAC_GROUP_testing.txt') / 1048576, 2), 'MB')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora procedemos a iniciar la tokenización. Se van a modelar los inicios de sentencia como \\<s> y los finales como \\</s>. Los numeros se van a reemplazar por \\<NUM>. Finalmente, se van a reemplazar los términos con frecuencia 1 por \\<UNK>. Para este último paso se creó un diccionario que contiene la frecuencia de cada término en el corpus de training mediante el uso de la función Counter de la librería collections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de oraciones en 20N_GROUP_training:  340220\n",
            "[('Haywood', 1), ('sheffield-hallam', 1), ('uptodate', 1), ('termcap', 1), ('terminfo', 1), ('tilde', 1), ('per-user', 1), ('Xsetup_', 1), ('(~user', 1), ('~user', 1)]\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Se cargan los archivos de training\n",
        "with open('20N_GROUP_training.txt', 'r', encoding='utf-8', errors='ignore') as file:\n",
        "    news_texts_train = file.read()\n",
        "\n",
        "# Vamos a separar el corpus de train de 20N en sentencias\n",
        "news_texts_train = news_texts_train.split('. ')\n",
        "print('Cantidad de oraciones en 20N_GROUP_training: ', len(news_texts_train))\n",
        "\n",
        "# Vamos a recorrer cada sentencia y separarla en palabras para agregarlas a un diccionario de frecuencias\n",
        "news_word_counts_train = Counter()\n",
        "for sentence in news_texts_train:\n",
        "    # Se separa la sentencia en palabras\n",
        "    sentence = sentence.split(' ')\n",
        "    # Si el ultimo caracter de una palabra no es una letra, se quita\n",
        "    for i in range(len(sentence)):\n",
        "        if sentence[i] != \"\" and not sentence[i][-1].isalpha():\n",
        "            sentence[i] = sentence[i][:-1]\n",
        "    # Se agregan las palabras al diccionario de frecuencias\n",
        "    news_word_counts_train.update(sentence)\n",
        "\n",
        "print(news_word_counts_train.most_common()[:-10-1:-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de oraciones en BAC_GROUP_training:  7540764\n",
            "[('working/learning/reading', 1), ('Shimrat', 1), ('Irit', 1), ('theme!&nbsp', 1), ('Maelene', 1), ('excitted', 1), ('chimms', 1), ('taurens', 1), ('ARUGH!!', 1), ('sb510', 1)]\n"
          ]
        }
      ],
      "source": [
        "with open('BAC_GROUP_training.txt', 'r', encoding='utf-8', errors='ignore') as file:\n",
        "    blogs_texts_train = file.read()\n",
        "\n",
        "# Vamos a separar el corpus de train de BAC en sentencias\n",
        "blogs_texts_train = blogs_texts_train.split('. ')\n",
        "print('Cantidad de oraciones en BAC_GROUP_training: ', len(blogs_texts_train))\n",
        "\n",
        "# Vamos a recorrer cada sentencia y separarla en palabras para agregarlas a un diccionario de frecuencias\n",
        "blogs_word_counts_train = Counter()\n",
        "for sentence in blogs_texts_train:\n",
        "    # Se separa la sentencia en palabras\n",
        "    sentence = sentence.split(' ')\n",
        "    # Si el ultimo caracter de una palabra no es una letra, se quita\n",
        "    for i in range(len(sentence)):\n",
        "        if sentence[i] != \"\" and not sentence[i][-1].isalpha():\n",
        "            sentence[i] = sentence[i][:-1]\n",
        "    # Se agregan las palabras al diccionario de frecuencias\n",
        "    blogs_word_counts_train.update(sentence)\n",
        "\n",
        "print(blogs_word_counts_train.most_common()[:-10-1:-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de palabras en 20N_GROUP_training:  234086\n",
            "Cantidad de tokens en 20N_GROUP_training:  5261998\n"
          ]
        }
      ],
      "source": [
        "print('Cantidad de palabras en 20N_GROUP_training: ', len(news_word_counts_train))\n",
        "\n",
        "# Ahora se va a terminar de tokenizar el corpus de train de 20N\n",
        "news_tokens_train = []\n",
        "\n",
        "for sentence in news_texts_train:\n",
        "    # Primero se agrega el token de inicio de oracion\n",
        "    news_tokens_train.append('<s>')\n",
        "    # Se separa la sentencia en palabras\n",
        "    sentence = sentence.split(' ')\n",
        "    # Se van revisando las palabras de la sentencia\n",
        "    for word in sentence:\n",
        "        # Si la palabra es un numero, se agrega el token <NUM>\n",
        "        if word.isnumeric():\n",
        "            news_tokens_train.append('<NUM>')\n",
        "        # Si la palabra no es vacia\n",
        "        elif word != \"\":\n",
        "            # Si la palabra termina en algo que no sea una letra o un numero\n",
        "            # se quita el ultimo caracter de la palabra y se agregan las dos partes\n",
        "            if not (word[-1].isalpha() or word[-1].isnumeric()):\n",
        "                # Si la palabra restante es un numero, se agrega el token <NUM>\n",
        "                if word[:-1].isnumeric():\n",
        "                    news_tokens_train.append('<NUM>')\n",
        "                # Si la palabra restante tiene frecuencia 1 o menos, se agrega el token <UNK>\n",
        "                elif news_word_counts_train[word[:-1]] <= 1: # Se hace así por si la palabra restante tiene frecuencia 0 (no existe)\n",
        "                    news_tokens_train.append('<UNK>')\n",
        "                else:\n",
        "                    news_tokens_train.append(word[:-1])\n",
        "                news_tokens_train.append(word[-1])\n",
        "            # Si su frecuencia es 1 o menos, se agrega el token <UNK>\n",
        "            elif news_word_counts_train[word] <= 1:\n",
        "                news_tokens_train.append('<UNK>')\n",
        "            # Si su frecuencia es mayor a 1, se agrega la palabra\n",
        "            else:\n",
        "                news_tokens_train.append(word)\n",
        "    # Se agrega el token de fin de oracion\n",
        "    news_tokens_train.append('</s>')\n",
        "\n",
        "print('Cantidad de tokens en 20N_GROUP_training: ', len(news_tokens_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de palabras en BAC_GROUP_training:  1565579\n"
          ]
        }
      ],
      "source": [
        "print('Cantidad de palabras en BAC_GROUP_training: ', len(blogs_word_counts_train))\n",
        "\n",
        "# Ahora se va a terminar de tokenizar el corpus de train de BAC\n",
        "blogs_tokens_train = []\n",
        "\n",
        "for sentence in blogs_texts_train:\n",
        "    # Primero se agrega el token de inicio de oracion\n",
        "    blogs_tokens_train.append('<s>')\n",
        "    # Se separa la sentencia en palabras\n",
        "    sentence = sentence.split(' ')\n",
        "    # Se van revisando las palabras de la sentencia\n",
        "    for word in sentence:\n",
        "        # Si la palabra es un numero, se agrega el token <NUM>\n",
        "        if word.isnumeric():\n",
        "            blogs_tokens_train.append('<NUM>')\n",
        "        # Si la palabra no es vacia\n",
        "        elif word != \"\":\n",
        "            # Si la palabra termina en algo que no sea una letra o un numero\n",
        "            # se quita el ultimo caracter de la palabra y se agregan las dos partes\n",
        "            if not (word[-1].isalpha() or word[-1].isnumeric()):\n",
        "                # Si la palabra restante es un numero, se agrega el token <NUM>\n",
        "                if word[:-1].isnumeric():\n",
        "                    blogs_tokens_train.append('<NUM>')\n",
        "                # Si la palabra restante tiene frecuencia 1 o menos, se agrega el token <UNK>\n",
        "                elif blogs_word_counts_train[word[:-1]] <= 1: # Se hace así por si la palabra restante tiene frecuencia 0 (no existe)\n",
        "                    blogs_tokens_train.append('<UNK>')\n",
        "                else:\n",
        "                    blogs_tokens_train.append(word[:-1])\n",
        "                blogs_tokens_train.append(word[-1])\n",
        "            # Si su frecuencia es 1 o menos, se agrega el token <UNK>\n",
        "            elif blogs_word_counts_train[word] <= 1:\n",
        "                blogs_tokens_train.append('<UNK>')\n",
        "            # Si su frecuencia es mayor a 1, se agrega la palabra\n",
        "            else:\n",
        "                blogs_tokens_train.append(word)\n",
        "    # Se agrega el token de fin de oracion\n",
        "    blogs_tokens_train.append('</s>')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<s>', \"I've\", 'been', 'selling', 'books', 'online', 'since', 'April', ',', 'and', 'am', 'painfully', 'aware', 'of', 'the', 'vast', 'ocean', 'of', 'book-knowledge', 'that', 'I', \"haven't\", 'yet', 'begun', 'to', 'cross', '</s>', '<s>', 'But', 'even', 'before', 'I', 'set', 'out', 'on', 'that', 'voyage', ',', \"there's\", 'a', 'smaller', 'body', 'of', 'retailing', 'commonsense', 'I', 'ought', 'to', 'master', '</s>', '<s>', 'Alas', 'for', 'me', '</s>', '<s>', \"Who'd\", 'have', 'guessed', 'that', \"there's\", 'a', 'person', 'out', 'there', 'who', 'uses', 'orange', 'highlighter', 'in', 'an', 'otherwise', 'lovely', 'urlLink', 'Heritage', 'Press', 'edition', 'of', 'Christopher', 'Marlowe', '?', \"What'd\", 'they', 'think', 'it', 'was', ',', 'a', 'textbook', '?', 'Lesson', '#1', ':', \"Don't\", 'wait', 'until', \"you're\", 'ready', 'to', 'sell', 'the', 'merchandise', 'to', 'give', 'it', 'a', 'careful', 'inspection', ',', 'even', 'if', 'it', 'looks', 'great', 'on', 'the', 'outside', '</s>', '<s>', 'Welcome', 'to', 'the', 'blog', 'of', 'urlLink', '<UNK>', 'books', '</s>', '<s>', \"I'm\", 'launching', 'this', 'blog', 'as', 'a', 'way', 'to', 'track', 'my', 'learning', 'curve', 'in', 'the', 'bookselling', 'business', ';', 'I', 'hope', 'it', 'will', 'be', 'fun', ',', 'useful', ',', 'and', 'above', 'all', ',', '<UNK>', 'plan', 'to', 'record', 'my', 'failures', 'and', 'stupidities', 'as', 'well', 'as', 'my', 'successes', 'and', 'insights', '</s>', '<s>', \"We'll\", 'be', 'right', 'back', 'after', 'this', 'word', 'from', 'our', 'sponsor', '</s>', '<s>', '</s>', '<s>', 'before', 'the', '<UNK>', 'nincompoops', 'started', 'urlLink', 'whining', 'about', 'Google', '</s>', '<s>', 'Choice', 'quote', ':', 'Behind', \"Google's\", 'complex', 'ranking', 'system', 'is', 'a', 'simple', 'idea', ':', 'each', 'link', 'to', 'a', 'page', 'should', 'be', 'considered', 'a', 'vote', ',', 'and', 'the', 'pages', 'with', 'the', 'most', 'votes', 'should', 'be', 'ranked', 'first', '</s>', '<s>', 'This', 'elegant', 'approach', 'uses', 'the', 'distributed', 'intelligence', 'of', 'Web', 'users', 'to', 'determine', 'which', 'content', 'is', 'most', 'relevant', '</s>', '<s>', 'But', 'what', 'is', 'good', 'for', 'Google', 'is', 'not', 'necessarily', 'good', 'for', 'the', 'rest', 'of', 'the', 'Web', '</s>', '<s>', 'Yeah', ',', 'God', 'forbid', 'that', 'the', 'distributed', 'intelligence', 'of', 'web', 'users', 'should', 'influence', 'web', 'site', 'popularity', '</s>', '<s>', 'I', 'demand', 'more', 'votes', 'for', 'the', 'little', 'web', 'site', '!', 'Popular', 'web', 'sites', 'should', 'be', 'made', 'to', 'hand', 'over', 'some', 'of', 'their', '<UNK>', 'got', 'enough', 'for', 'all', 'of', 'us', '!', 'Ooooh', ',', 'and', 'you', 'know', 'what', '?', '(Cue', 'foreboding', 'Fahrenheit', '9/11', 'music', ')', 'I', 'did', 'a', 'Google', 'search', 'on', '\"Dick', 'Cheney', '\"', 'and', 'got', 'over', 'one', 'million', 'hits', '!!', '!', 'I', 'smell', 'a', 'cover-up', '</s>', '<s>', 'Read', 'this', 'young', \"woman's\", 'essay', 'on', '', '\"', 'urlLink', 'Why', 'I', 'am', 'not', 'a', 'Republican', '</s>', '<s>', '', '\"', 'Then', 'read', 'the', 'rest', 'of', 'urlLink', 'her', 'blog', '</s>', '<s>', 'Then', 'tell', 'me', 'you', \"don't\", 'love', 'her', '</s>', '<s>', 'Wow', '!', 'Take', 'the', 'Christian', 'Science', 'Monitor', 'urlLink', 'quiz', 'and', 'find', 'out', '</s>', '<s>', 'My', 'results', 'make', 'me', 'out', 'to', 'be', 'a', '<UNK>', '</s>', '<s>', '', '\"', 'Realists', ':', 'Are', 'guided', 'more', 'by', 'practical', 'considerations', 'than', 'ideological', 'vision', 'Believe', 'US', 'power', 'is', 'crucial', 'to', 'successful', 'diplomacy', '', '-', 'and', 'vice', 'versa', \"Don't\", 'want', 'US', 'policy', 'options', 'unduly', 'limited', 'by', 'world', 'opinion', 'or', 'ethical', 'considerations', 'Believe', 'strong', 'alliances', 'are', 'important', 'to', 'US', 'interests', 'Weigh', 'the', 'political', 'costs', 'of', 'foreign', 'action', 'Believe', 'foreign', 'intervention', 'must', 'be', 'dictated', 'by', 'compelling', 'national', 'interest', 'Historical', 'realist', ':', 'President', 'Dwight', 'D', '</s>', '<s>', 'Eisenhower', 'Modern', 'realist', ':', 'Secretary', 'of', 'State', 'Colin', 'Powell', 'So', 'I', 'wasted', 'a', 'whole', 'Saturday', 'futzing', 'around', 'on', 'the', 'computer', ';', \"what's\", 'your', 'point', '?', 'Anyway', ',', 'I', 'just', 'bought', 'an', 'entry-level', 'digital', 'camera', 'to', 'take', 'pictures', 'of', 'sets', 'of', 'books', '(too', 'hard', 'to', 'get', 'a', 'dozen', 'of', 'them', 'to', 'sit', 'still', 'on', 'the', 'scanner', ')', '</s>', '<s>', 'Thought', \"I'd\", 'try', 'it', 'out', 'on', 'Daisy', ',', 'our', 'five-year-old', 'housemate', ':', 'The', 'cats', 'have', 'taught', 'her', 'to', 'affect', 'an', 'air', 'of', 'indifference', ',', 'but', 'I', 'can', 'tell', \"she's\", 'secretly', 'impressed', '</s>', '<s>', 'Taking', 'a', 'page', 'from', 'urlLink', 'Keith', '<UNK>', '', ',', \"I'm\", 'going', 'to', 'post', 'quotes', 'from', 'academic', 'journal', 'articles', 'and', 'books', 'that', 'strike', 'me', 'as', 'absurd', ',', 'outrageous', ',', 'reality-challenged', ',', 'or', 'some', 'tasty', 'combination', 'thereof', '</s>', '<s>', 'As', 'I', 'lack', 'his', 'philosophical', 'charity', ',', 'however', ',', 'I', \"won't\", 'be', 'able', 'to', 'refrain', 'from', 'appending', 'snarky', 'comments', 'to', 'them', '</s>', '<s>', \"Here's\", 'the', 'inaugural', '<UNK>', ':', 'The', 'already', 'ambiguous', 'conflation', 'of', 'nation-state', '(Soviet', ')', 'and', 'ideology', '<UNK>', ')', 'in', 'the', 'portrayal', 'of', 'enemies', 'abstracted', 'and', 'covered', 'over', 'a', '<UNK>', 'set', 'of', 'repressions', 'that', 'not', 'only', 'silenced', 'women', ',', 'but', 'also', 'others', 'colonized', 'by', 'national', 'projects', ',', 'whose', 'objections', 'or', 'affiliations', 'might', 'bring', 'them', 'fatally', 'back', 'into', 'view', '</s>', '<s>', 'The', 'subject', 'of', 'the', 'Cold', 'War', 'was', 'military', ',', 'although', 'the', 'war', 'enlisted', 'all', 'of', 'everyday', 'life', '</s>', '<s>', 'This', 'had', 'the', 'effect', 'of', 'making', 'all', 'other', 'discourses', 'seem', 'not', 'only', 'internal', ',', 'but', 'also', 'more', 'subjective', ',', 'more', 'unreal', 'as', 'more', 'distant', 'from', 'the', 'military', 'project', '</s>', '<s>', 'Even', 'the', 'struggle', 'between', 'science', 'and', 'the', 'national', 'interest', 'was', 'not', 'won', 'by', 'science', ',', 'and', 'those', 'scientists', 'who', 'insisted', 'on', 'sharing', 'atomic', 'secrets', 'could', 'be', 'put', 'to', 'death', 'as', 'spies', '</s>', '<s>', 'Maximum', 'objectivity', 'and', 'realism', 'was', 'not', 'attributed', 'to', 'the', 'scientists', 'who', 'advocated', 'open', 'international', 'inquiry', ',', 'but', 'rather', 'to', 'the', 'bureaucracy', 'that', 'took', 'away', 'their', 'passports', '</s>', '<s>', '<UNK>', 'of', 'some', 'previews', 'of', 'chapter', '<NUM>', ']', 'Surely', 'the', 'Rosenberg', 'executions', 'enforced', 'not', 'just', 'power', 'but', 'belief', 'in', 'the', 'possibility', 'of', 'subversion', 'through', 'sharing', 'of', 'scientific', 'knowledge', ',', 'now', 'conflated', 'with', 'the', 'development', 'of', 'weaponry', '</s>', '<s>', 'The', 'stakes', 'were', 'the', 'highest', ':', 'the', 'arms', 'race', 'cost', 'more', 'than', 'both', 'world', 'wars', 'combined', '</s>', '<s>', '<UNK>', '\"', 'became', 'a', 'standard', 'of', 'proof', 'and', 'mark', 'of', 'suspicion', 'shared', 'by', 'nuclear', 'weapons', 'negotiations', 'and', 'the', 'philosophy', 'of', 'science', '(9', ')', '</s>', '<s>', '<UNK>', ',', 'Suzanne', '</s>', '<s>', 'Cold', 'Warriors', ':', 'Manliness', 'on', 'Trial', 'in', 'the', 'Rhetoric', 'of', 'the', 'West', '</s>', '<s>', 'Carbondale', ':', 'Southern', 'Illinois', 'UP', ',', '<NUM>', '</s>', '<s>', 'I', '<UNK>', '-', 'grieve', '<UNK>', 'the', 'cruel', 'fate', 'suffered', 'by', 'those', 'starry-eyed', 'scientists', 'whose', 'passports', 'were', 'revoked', 'by', 'a', 'hypermasculine', ',', '<UNK>', ',', '<UNK>', 'idolizing', ',', 'war-mongering', '\"United', '\"', '\"States', '\"', '\"culture', '</s>', '<s>', '', '\"', 'The', 'world', 'would', 'have', 'been', 'much', 'better', 'off', 'had', 'their', 'attempts', 'to', 'sustain', '\"open', 'inquiry', '\"', 'succeeded', '</s>', '<s>', 'And', 'the', 'brave', 'Rosenbergs', 'were', 'just', 'making', 'a', 'desperate', 'bid', 'for', 'peace', ',', 'or', 'at', 'least', 'fiscal', 'prudence', '(seeing', 'as', 'how', 'the', 'arms', 'race', 'was', 'so', 'much', 'more', 'expensive', 'than', 'both', 'world', 'wars', 'combined', ')', '</s>', '<s>', 'How', 'I', 'wish', \"we'd\", 'listened', 'to', 'the', '<UNK>', ';', 'had', 'the', 'Soviets', 'prevailed', ',', 'the', 'national', 'project', 'colonizing', 'us', 'would', 'have', 'been', 'complete', 'and', 'total', ',', 'and', 'therefore', 'much', 'more', 'egalitarian', 'than', 'the', 'selective', 'colonization', 'of', 'the', 'tragically', 'unhip', 'Cold', 'War', 'era', '</s>', '<s>', 'One', 'of', 'my', 'favorite', 'blogs', 'urlLink', 'passes', 'the', 'one', 'year', 'mark', 'today', '</s>', '<s>', 'Alex', 'Tabarrok', 'and', 'Tyler', 'Cowen', 'have', 'earned', 'the', 'right', 'to', 'crow', ';', 'readers', 'are', 'guaranteed', 'to', 'always', 'learn', 'something', 'fascinating', 'from', 'them', '</s>', '<s>', 'The', 'Winter', '2003-2004', 'issue', 'of', 'Academic', 'Questions', '', ',', 'the', 'journal', 'of', 'the', 'urlLink', 'National', 'Association', 'of', 'Scholars', '--check', 'them', 'out', 'if', 'you', \"don't\", 'know', 'who', 'they', '<UNK>', 'in', 'my', 'mailbox', 'yesterday', '(so', \"it's\", 'a', 'little', 'late', ')', '</s>', '<s>', 'In', 'it', ',', 'urlLink', 'Mark', 'Bauerlein', 'writes', 'a', 'tough', 'review', 'of', 'Gerald', \"Graff's\", 'book', ',', 'urlLink', 'Clueless', 'in', 'Academe', ':', 'How', 'Schooling', '<UNK>', 'the', 'Life', 'of', 'the', 'Mind', '</s>', '<s>', 'Graff', 'is', 'probably', 'best', 'known', 'for', 'his', 'book', 'urlLink', 'Teaching', 'the', 'Conflicts', '', ',', 'which', 'argues', 'that', 'poststructuralist', '\"disciplinary', '\"', 'hypertrophy', \"doesn't\", 'have', 'to', 'leave', 'students', 'behind', '</s>', '<s>', 'As', 'an', 'earnest', 'graduate', 'student', 'looking', 'for', 'guidance', 'in', 'teaching', ',', 'I', 'found', \"Graff's\", 'approach', 'lucid', 'but', 'strangely', 'tepid', 'and', 'unsatisfying', ',', 'although', 'I', \"didn't\", 'really', 'analyze', 'why', '</s>', '<s>', 'Bauerlein', ',', 'on', 'the', 'other', 'hand', ',', 'sticks', 'his', 'scalpel', 'right', 'into', 'the', 'bad', 'spots', '</s>', '<s>', 'I', 'wish', 'I', 'could', 'link', 'to', 'an', 'online', 'version', 'of', 'the', 'review', ',', 'or', 'scan', 'it', 'in', 'for', 'you', ',', 'but', 'that', 'would', 'be', 'wrong', ',', 'so', \"I'll\", 'just', 'quote', 'some', 'choice', 'bits', ':', '<UNK>', ']', 'name', 'echoes', 'of', 'past', 'controversies', 'and', 'his', 'book', 'sports', 'an', '<UNK>', 'title', ',', 'but', 'whether', 'a', 'noted', 'professor', 'in', 'his', 'twilight', 'can', 'muster', 'the', 'courage', 'and', '<UNK>', 'to', 'carry', 'out', 'a', 'thorough', 'professional', 'soul-searching', 'is', 'doubtful', '</s>', '<s>', 'Graff', 'answers', 'the', 'question', 'in', 'his', 'very', 'first', 'sentence', ':', 'This', 'book', 'is', 'an', 'attempt', 'by', 'an', 'academic', 'to', 'look', 'at', 'academia', 'from', 'the', 'perspective', 'of', 'those', 'who', \"don't\", 'get', 'it', '</s>', '<s>', 'It', 'sounds', 'good', 'until', 'you', 'reach', 'the', 'last', 'three', 'words', '</s>', '<s>', 'To', 'see', 'academe', 'from', 'outside', 'the', 'campus', 'walls', 'and', 'faculty', 'cliques', 'is', 'a', '<UNK>', 'aim', '</s>', '<s>', 'If', 'humanities', 'professors', 'made', 'the', 'least', 'effort', 'to', 'understand', 'why', 'Eugene', '<UNK>', ',', 'Frederick', 'Crews', ',', 'and', 'others', 'criticize', 'the', 'field', ',', 'instead', 'of', 'dismissing', 'them', 'as', 'reactionaries', 'and', 'dumbbells', ',', 'they', 'might', 'strengthen', 'their', 'own', 'positions', 'or', 'even', 'find', 'points', 'of', 'agreement', '</s>', '<s>', 'But', 'the', '\"don\\'t', 'get', 'it', '\"', 'phrase', 'shortchanges', 'the', 'outsiders', '</s>', '<s>', 'Even', 'though', 'it', 'suggests', 'some', 'sympathy', 'for', 'them', ',', 'they', 'are', 'still', 'the', 'benighted', ',', 'the', 'confused', ',', 'people', 'who', 'feel', '\"shame', 'and', 'resentment', '\"', 'when', 'facing', 'the', '<UNK>', 'of', 'the', 'academic', 'world', '\"', '(91', ')', '</s>', '<s>', '', '[', '</s>', '<s>', '</s>', '<s>', '</s>', '<s>', '', ']', 'But', 'look', 'closely', 'at', 'at', \"Graff's\", 'diagnosis', 'of', 'the', 'problems', 'and', 'you', 'find', 'an', 'evasion', 'and', 'a', 'rationalization', 'wholly', 'consistent', 'with', 'the', 'dithering', 'appraisals', 'of', 'his', 'generation', '</s>', '<s>', 'The', 'runaround', 'is', 'simple', ':', 'Graff', 'attributes', 'the', 'breakdown', 'of', 'humanities', 'education', 'solely', 'to', 'a', 'rhetorical', 'failure', '</s>', '<s>', 'Scholars', 'and', 'teachers', 'think', 'sharply', 'and', 'reason', 'skillfully', ',', 'he', 'insists', ',', 'but', 'they', \"don't\", 'express', 'their', 'ideas', 'in', 'limpid', 'speech', '</s>', '<s>', 'Advances', 'in', 'curriculum', 'proceed', ',', 'breakthroughs', 'of', 'theory', 'have', 'transpired', ',', 'but', 'the', 'academic', 'idiom', \"hasn't\", 'articulated', 'them', 'well', '</s>', '<s>', 'Students', 'shy', 'away', 'only', 'because', 'they', \"don't\", 'speak', 'the', 'professors', \"'\", 'language', '</s>', '<s>', 'In', 'class', ',', '\"Once', 'students', 'have', 'to', 'translate', 'their', 'personal', 'interests', 'and', 'experience', 'into', 'the', 'formalized', 'conventions', 'of', 'written', 'Arguespeak', ',', 'their', 'interests', 'and', 'experience', 'no', 'longer', 'seem', 'their', 'own', '</s>', '<s>', '', '\"', 'Nothing', 'in', 'the', 'values', ',', 'principles', ',', 'and', 'knowledge', 'of', 'the', 'professors', 'is', 'askew', '</s>', '<s>', 'It', 'is', ',', 'rather', ',', 'only', 'the', 'communication', 'that', 'needs', 'fixing', '</s>', '<s>', 'The', 'humanities', 'are', 'dying', ',', 'but', 'Graff', 'goes', 'no', 'further', 'than', 'urging', ',', '\"We', 'have', 'to', 'improve', 'our', 'message', '\"', '(93', ')', '</s>', '<s>', '', '[', '</s>', '<s>', '</s>', '<s>', '</s>', '<s>', '', ']', 'To', 'consider', 'an', '<UNK>', 'with', 'Arguespeak', 'the', 'prime', 'shortcoming', 'of', 'students', 'is', 'to', 'overlook', 'other', ',', 'gaping', 'deficiencies', 'of', 'skill', 'and', 'knowledge', '</s>', '<s>', 'Graff', 'appears', 'unconcerned', 'with', 'the', 'fact', 'that', 'high', 'school', 'graduates', \"can't\", 'write', 'a', 'periodic', 'sentence', ',', 'barely', 'understand', 'a', 'passage', 'of', 'prose', ',', 'disregard', 'the', 'classics', ',', 'and', \"can't\", 'pinpoint', 'the', 'half-century', 'in', 'which', 'the', 'Civil', 'War', 'took', 'place', '</s>', '<s>', 'Feeble', 'historical', 'learning', ',', 'declining', 'reading', 'scores', ',', 'pitiful', 'writing', '<UNK>', 'give', 'place', 'to', 'a', 'particular', 'forensic', ',', 'Arguespeak', '</s>', '<s>', 'Humanities', 'education', 'is', 'training', 'in', 'academic', 'discourse', ',', 'not', 'the', 'study', 'of', 'history', 'and', 'literature', '</s>', '<s>', 'Rather', 'than', 'forming', 'students', 'into', 'learned', ',', 'eloquent', 'minds', ',', \"Graff's\", 'pedagogy', 'shapes', 'them', 'into', 'canny', '<UNK>', ',', 'that', 'is', ',', 'into', 'junior', 'imitations', 'of', 'their', 'professors', '(94', ')', '</s>', '<s>', 'As', 'if', 'this', \"weren't\", 'enough', ',', '<UNK>', 'introductory', 'paragraphs', 'render', 'a', 'devastating', 'picture', 'of', 'the', 'rise', 'of', 'poststructuralist', 'scholars', ':', '<UNK>', 'scholars', 'in', 'the', '60s', 'and', '70s', ']', 'like', 'to', 'remember', 'those', 'years', 'as', 'an', 'uphill', 'struggle', 'against', 'old-fashioned', 'formalists', ',', 'biographers', ',', '<UNK>', ',', 'and', 'arts', '<UNK>', ',', 'but', 'I', 'have', 'yet', 'to', 'hear', 'of', 'a', 'hotshot', 'iconoclast', 'skilled', 'in', 'structuralism', 'who', 'suffered', 'for', 'his', 'beliefs', '</s>', '<s>', 'The', 'fact', 'is', ',', 'campus', 'conditions', 'favored', 'them', '</s>', '<s>', 'As', 'administrators', 'caved', 'in', 'to', 'various', 'protesters', ',', 'they', 'found', 'a', 'rationalization', 'in', 'the', '<UNK>', 'tailwind', 'and', 'jumped', 'to', 'invest', 'in', 'the', 'bold', 'new', 'critical', 'world', '</s>', '<s>', '', '[', '</s>', '<s>', '</s>', '<s>', '</s>', '<s>', '', ']', 'Life', 'was', 'good', ',', 'and', 'one', 'can', 'hardly', 'blame', 'the', 'beneficiaries', 'if', 'they', 'attributed', 'the', 'largesse', 'to', 'their', 'own', 'abilities', 'instead', 'of', 'to', 'post-War', 'demographics', 'and', 'social', 'movements', '</s>', '<s>', 'Why', 'should', 'a', 'cutting-edge', 'specialist', 'in', 'French', 'thought', 'ask', 'about', 'the', 'social', 'value', 'of', 'literary', 'theory', 'when', 'so', 'many', 'were', 'demanding', 'his', 'attention', '?', 'A', 'Marxist', 'professor', 'invited', 'to', 'lecture', 'from', 'Sydney', 'to']\n"
          ]
        }
      ],
      "source": [
        "print(blogs_tokens_train[:2000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de palabras en 20N_GROUP_training:  234086\n",
            "[('20-4', 1), ('entre', 1), ('tu', 1), ('tienes', 1), ('¿Cuantos', 1), ('2', 1), ('parece', 1), ('Tambien', 1), ('años', 2), ('Tengo', 2)]\n",
            "['<s>', 'Hola', '!', 'soy', 'Daniel', 'Osorio', 'y', 'me', 'gusta', 'jugar', 'al', 'fubol', '</s>', '<s>', 'Hola', '!', 'soy', 'Daniel', 'Osorio', 'y', 'me', 'gusta', 'jugar', 'al', 'fubol', '</s>', '<s>', '<UNK>', 'me', '<UNK>', 'gusta', 'jugar', '?', '</s>', '<s>', 'Tengo', '<NUM>', 'años', 'y', 'me', 'gusta', 'jugar', 'al', 'fubol', '</s>', '<s>', '<UNK>', 'años', '<UNK>', '<UNK>', '?', '</s>', '<s>', 'Tengo', '<UNK>', '<UNK>', '</s>']\n"
          ]
        }
      ],
      "source": [
        "print('Cantidad de palabras en 20N_GROUP_training: ', len(news_word_counts_train))\n",
        "\n",
        "# Se va a hacer una prueba de cómo se tokeniza una oración\n",
        "\n",
        "# Primero generamos varias oraciones con palabras repetidas y con caracteres especiales, y con algunas palabras únicas\n",
        "test_sentences = [  'Hola! soy Daniel Osorio y me gusta jugar al fubol',\n",
        "                    'Hola! soy Daniel Osorio y me gusta jugar al fubol',\n",
        "                    \"Tambien me parece gusta jugar?\",\n",
        "                    \"Tengo 20 años y me gusta jugar al fubol\",\n",
        "                    \"¿Cuantos años tienes tu?\",\n",
        "                    \"Tengo entre 20-40\"]\n",
        "\n",
        "# Se obtienen las frecuencias de cada palabra\n",
        "test_word_counts = Counter()\n",
        "for sentence in test_sentences:\n",
        "    # Se separa la sentencia en palabras\n",
        "    sentence = sentence.split(' ')\n",
        "    # Si el ultimo caracter de una palabra no es una letra, se quita\n",
        "    for i in range(len(sentence)):\n",
        "        if sentence[i] != \"\" and not sentence[i][-1].isalpha():\n",
        "            sentence[i] = sentence[i][:-1]\n",
        "    # Se agregan las palabras al diccionario de frecuencias\n",
        "    test_word_counts.update(sentence)\n",
        "\n",
        "print(test_word_counts.most_common()[:-10-1:-1])\n",
        "\n",
        "# Ahora se va a tokenizar cada oración\n",
        "test_tokens = []\n",
        "for sentence in test_sentences:\n",
        "    # Primero se agrega el token de inicio de oracion\n",
        "    test_tokens.append('<s>')\n",
        "    # Se separa la sentencia en palabras\n",
        "    sentence = sentence.split(' ')\n",
        "    # Se van revisando las palabras de la sentencia\n",
        "    for word in sentence:\n",
        "        # Si la palabra es un numero, se agrega el token <NUM>\n",
        "        if word.isnumeric():\n",
        "            test_tokens.append('<NUM>')\n",
        "        # Si la palabra no es vacia\n",
        "        elif word != \"\":\n",
        "            # Si la palabra termina en algo que no sea una letra, \n",
        "            # se quita el ultimo caracter de la palabra y se agregan las dos partes\n",
        "            if not (word[-1].isalpha() or word[-1].isnumeric()):\n",
        "                # Si la palabra restante es un numero, se agrega el token <NUM>\n",
        "                if word[:-1].isnumeric():\n",
        "                    test_tokens.append('<NUM>')\n",
        "                # Si la palabra restante tiene frecuencia 1 o menos, se agrega el token <UNK>\n",
        "                elif test_word_counts[word[:-1]] <= 1: # Se hace así por si la palabra restante tiene frecuencia 0 (no existe)\n",
        "                    test_tokens.append('<UNK>')\n",
        "                else:\n",
        "                    test_tokens.append(word[:-1])\n",
        "                test_tokens.append(word[-1])\n",
        "            # Si su frecuencia es 1 o menos, se agrega el token <UNK>\n",
        "            elif test_word_counts[word] <= 1:\n",
        "                test_tokens.append('<UNK>')\n",
        "            # Si su frecuencia es mayor a 1, se agrega la palabra\n",
        "            else:\n",
        "                test_tokens.append(word)\n",
        "    # Se agrega el token de fin de oracion\n",
        "    test_tokens.append('</s>')\n",
        "\n",
        "print(test_tokens)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
