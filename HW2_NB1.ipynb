{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdRxw48iK5pk"
      },
      "source": [
        "# **Procesamiento de Lenguaje Natural**\n",
        "\n",
        "## **Tarea 2 - Notebook 1**\n",
        "\n",
        "## Integrantes\n",
        "\n",
        "* ### Daniel Osorio Cárdenas\n",
        "* ### Juan Diego Calixto Núñez\n",
        "\n",
        "Este Notebook incluye los literales I, II, III, IV de la tarea. Estos corresponden a la construcción de los modelos de lenguaje que se usarán en el Notebook 2.\n",
        "\n",
        "## **I. Creación de los archivos consolidados**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7Z5cokMaKNPL"
      },
      "outputs": [],
      "source": [
        "# Configurar los directorios donde se encuentran los archivos de datos\n",
        "NEWS_FOLDER = '20news-18828'\n",
        "BLOGS_FOLDER = 'blogs'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Empezamos cargando los archivos de 20news-18828 y creamos un único archivo consolidado llamado \"20N_consolidated.txt\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tamaño del archivo 20N_consolidated.txt:  31568937 bytes =  30.11 MB\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Listar los folders de 20news-18828\n",
        "news_folder = os.listdir(NEWS_FOLDER)\n",
        "\n",
        "# Recorremos cada archivo de news\n",
        "news_texts = []\n",
        "for folder in news_folder:\n",
        "    for file_name in os.listdir(NEWS_FOLDER + '/' + folder):\n",
        "        extracted_text = ''\n",
        "        with open(NEWS_FOLDER + '/' + folder + '/' + file_name, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                # Agregamos solo las lineas del contenido del archivo\n",
        "                if not line.startswith('From:') and not line.startswith('Subject:') and not line.startswith('Fax:') and not line.startswith('Phone:') and not line.startswith('Email:') and not line.startswith('INTERNET:'):\n",
        "                    extracted_text += line + \" \"\n",
        "            news_texts.append(extracted_text)\n",
        "\n",
        "# Guardamos todos los textos en un unico .txt\n",
        "with open('20N_consolidated.txt', 'w', encoding='utf-8') as file:\n",
        "    for text in news_texts:\n",
        "        file.write(text)\n",
        "\n",
        "# Revisamos el tamaño del archivo .txt\n",
        "print('Tamaño del archivo 20N_consolidated.txt: ', os.path.getsize('20N_consolidated.txt'), 'bytes = ', round(os.path.getsize('20N_consolidated.txt') / 1048576, 2), 'MB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para cargar los archivos de blogs se hizo primero un preprocesamiento de los archivos XML puesto que hubo muchos errores a la hora de leerlos. La estrategia fue la siguiente:\n",
        "* Se eliminaron los tags de formato del XML\n",
        "* Se eliminaron las fechas de los posts\n",
        "* Se dejó unicamente el contenido de los posts\n",
        "\n",
        "Esto para poder leer los archivos como texto plano. El archivo que hace esta modificación se llama \"blogs_preprocessing.py\".\n",
        "\n",
        "Finalmente, se cargaron los archivos de blogs y se creó un único archivo consolidado llamado \"BAC_consolidated.txt\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tamaño del archivo BAC_consolidated.txt:  757389894 bytes =  722.3 MB\n"
          ]
        }
      ],
      "source": [
        "# Recorremos cada archivo de blogs\n",
        "blogs_texts = []\n",
        "for filename in os.listdir(BLOGS_FOLDER):\n",
        "    extracted_text = ''\n",
        "    with open(BLOGS_FOLDER + '/' + filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            # Agregamos solo las lineas del contenido del archivo\n",
        "            if not line.startswith('From:') and not line.startswith('Subject:') and not line.startswith('Fax:') and not line.startswith('Phone:') and not line.startswith('Email:') and not line.startswith('INTERNET:'):\n",
        "                extracted_text += line + \" \"\n",
        "        blogs_texts.append(extracted_text)\n",
        "\n",
        "# Guardamos todos los textos en un unico .txt\n",
        "with open('BAC_consolidated.txt', 'w', encoding='utf-8') as file:\n",
        "    for text in blogs_texts:\n",
        "        file.write(text)\n",
        "\n",
        "# Revisamos el tamaño del archivo .txt\n",
        "print('Tamaño del archivo BAC_consolidated.txt: ', os.path.getsize('BAC_consolidated.txt'), 'bytes = ', round(os.path.getsize('BAC_consolidated.txt') / 1048576, 2), 'MB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **II y III. Crear archivos de Training y Test y Tokenizar por sentencia**\n",
        "\n",
        "Primero se crean los archivos de training y test. Para esto se separó cada archivo consolidado por oraciones. Suponemos que las oraciones están separadas por puntos. Se crearon dos archivos por cada corpus, uno de training y otro de test. Los archivos de training contienen el 80% de las oraciones y los de test el 20% restante."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de oraciones en 20news-18828:  425274\n",
            "Cantidad de oraciones en BAC:  9425954\n",
            "Tamaño del archivo .txt de 20N_GROUP_training:  25266186 bytes =  24.1 MB\n",
            "Tamaño del archivo .txt de 20N_GROUP_testing:  6265196 bytes =  5.97 MB\n",
            "Tamaño del archivo .txt de BAC_GROUP_training:  599507899 bytes =  571.74 MB\n",
            "Tamaño del archivo .txt de BAC_GROUP_testing:  148491073 bytes =  141.61 MB\n"
          ]
        }
      ],
      "source": [
        "# Cargamos los textos de los archivos .txt\n",
        "with open('20N_consolidated.txt', 'r', encoding='utf-8', errors='ignore') as file:\n",
        "    news_texts = file.read()\n",
        "\n",
        "with open('BAC_consolidated.txt', 'r', encoding='utf-8', errors='ignore') as file:\n",
        "    blogs_texts = file.read()\n",
        "\n",
        "# Separar cada archivo en oraciones\n",
        "news_texts = news_texts.split('.')\n",
        "blogs_texts = blogs_texts.split('.')\n",
        "\n",
        "# Quitar los textos vacios\n",
        "news_texts = [text for text in news_texts if text != '']\n",
        "blogs_texts = [text for text in blogs_texts if text != '']\n",
        "\n",
        "print('Cantidad de oraciones en 20news-18828: ', len(news_texts))\n",
        "print('Cantidad de oraciones en BAC: ', len(blogs_texts))\n",
        "\n",
        "# Dividir los textos en 80% train y 20% test\n",
        "news_texts_train = news_texts[:int(len(news_texts) * 0.8)]\n",
        "news_texts_test = news_texts[int(len(news_texts) * 0.8):]\n",
        "blogs_texts_train = blogs_texts[:int(len(blogs_texts) * 0.8)]\n",
        "blogs_texts_test = blogs_texts[int(len(blogs_texts) * 0.8):]\n",
        "\n",
        "# Guardar los textos de train y test en archivos .txt\n",
        "with open('20N_GROUP_training.txt', 'w', encoding='utf-8') as file:\n",
        "    for text in news_texts_train:\n",
        "        file.write(text.strip() + '. ')\n",
        "\n",
        "with open('20N_GROUP_testing.txt', 'w', encoding='utf-8') as file:\n",
        "    for text in news_texts_test:\n",
        "        file.write(text.strip() + '. ')\n",
        "\n",
        "with open('BAC_GROUP_training.txt', 'w', encoding='utf-8') as file:\n",
        "    for text in blogs_texts_train:\n",
        "        file.write(text.strip() + '. ')\n",
        "\n",
        "with open('BAC_GROUP_testing.txt', 'w', encoding='utf-8') as file:\n",
        "    for text in blogs_texts_test:\n",
        "        file.write(text.strip() + '. ')\n",
        "\n",
        "# Revisamos el tamaño de los archivos .txt\n",
        "print('Tamaño del archivo .txt de 20N_GROUP_training: ', os.path.getsize('20N_GROUP_training.txt'), 'bytes = ', round(os.path.getsize('20N_GROUP_training.txt') / 1048576, 2), 'MB')\n",
        "print('Tamaño del archivo .txt de 20N_GROUP_testing: ', os.path.getsize('20N_GROUP_testing.txt'), 'bytes = ', round(os.path.getsize('20N_GROUP_testing.txt') / 1048576, 2), 'MB')\n",
        "print('Tamaño del archivo .txt de BAC_GROUP_training: ', os.path.getsize('BAC_GROUP_training.txt'), 'bytes = ', round(os.path.getsize('BAC_GROUP_training.txt') / 1048576, 2), 'MB')\n",
        "print('Tamaño del archivo .txt de BAC_GROUP_testing: ', os.path.getsize('BAC_GROUP_testing.txt'), 'bytes = ', round(os.path.getsize('BAC_GROUP_testing.txt') / 1048576, 2), 'MB')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora procedemos a iniciar la tokenización. Se van a modelar los inicios de sentencia como \\<s> y los finales como \\</s>. Los numeros se van a reemplazar por \\<NUM>. Finalmente, se van a reemplazar los términos con frecuencia 1 por \\<UNK>. Para este último paso se creó un diccionario que contiene la frecuencia de cada término en el corpus de training mediante el uso de la función Counter de la librería collections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A continuación se muestra un ejemplo con sentencias de prueba para mostrar cómo se tokenizan los textos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('20-4', 1), ('entre', 1), ('tu', 1), ('tienes', 1), ('¿Cuantos', 1), ('2', 1), ('parece', 1), ('Tambien', 1), ('años', 2), ('Tengo', 2)]\n",
            "['<s>', 'Hola', '!', 'soy', 'Daniel', 'Osorio', 'y', 'me', 'gusta', 'jugar', 'al', 'fubol', '</s>', '<s>', 'Hola', '!', 'soy', 'Daniel', 'Osorio', 'y', 'me', 'gusta', 'jugar', 'al', 'fubol', '</s>', '<s>', '<UNK>', 'me', '<UNK>', 'gusta', 'jugar', '?', '</s>', '<s>', 'Tengo', '<NUM>', 'años', 'y', 'me', 'gusta', 'jugar', 'al', 'fubol', '</s>', '<s>', '<UNK>', 'años', '<UNK>', '<UNK>', '?', '</s>', '<s>', 'Tengo', '<UNK>', '<UNK>', '</s>']\n"
          ]
        }
      ],
      "source": [
        "# Se va a hacer una prueba de cómo se tokeniza una oración\n",
        "\n",
        "# Primero generamos varias oraciones con palabras repetidas y con caracteres especiales, y con algunas palabras únicas\n",
        "test_sentences = [  'Hola! soy Daniel Osorio y me gusta jugar al fubol',\n",
        "                    'Hola! soy Daniel Osorio y me gusta jugar al fubol',\n",
        "                    \"Tambien me parece gusta jugar?\",\n",
        "                    \"Tengo 20 años y me gusta jugar al fubol\",\n",
        "                    \"¿Cuantos años tienes tu?\",\n",
        "                    \"Tengo entre 20-40\"]\n",
        "\n",
        "# Se obtienen las frecuencias de cada palabra\n",
        "test_word_counts = Counter()\n",
        "for sentence in test_sentences:\n",
        "    # Se separa la sentencia en palabras\n",
        "    sentence = sentence.split(' ')\n",
        "    # Si el ultimo caracter de una palabra no es una letra, se quita\n",
        "    for i in range(len(sentence)):\n",
        "        if sentence[i] != \"\" and not sentence[i][-1].isalpha():\n",
        "            sentence[i] = sentence[i][:-1]\n",
        "    # Se agregan las palabras al diccionario de frecuencias\n",
        "    test_word_counts.update(sentence)\n",
        "\n",
        "print(test_word_counts.most_common()[:-10-1:-1])\n",
        "\n",
        "# Ahora se va a tokenizar cada oración\n",
        "test_tokens = []\n",
        "for sentence in test_sentences:\n",
        "    # Primero se agrega el token de inicio de oracion\n",
        "    test_tokens.append('<s>')\n",
        "    # Se separa la sentencia en palabras\n",
        "    sentence = sentence.split(' ')\n",
        "    # Se van revisando las palabras de la sentencia\n",
        "    for word in sentence:\n",
        "        # Si la palabra es un numero, se agrega el token <NUM>\n",
        "        if word.isnumeric():\n",
        "            test_tokens.append('<NUM>')\n",
        "        # Si la palabra no es vacia\n",
        "        elif word != \"\":\n",
        "            # Si la palabra termina en algo que no sea una letra, \n",
        "            # se quita el ultimo caracter de la palabra y se agregan las dos partes\n",
        "            if not (word[-1].isalpha() or word[-1].isnumeric()):\n",
        "                # Si la palabra restante es un numero, se agrega el token <NUM>\n",
        "                if word[:-1].isnumeric():\n",
        "                    test_tokens.append('<NUM>')\n",
        "                # Si la palabra restante tiene frecuencia 1 o menos, se agrega el token <UNK>\n",
        "                elif test_word_counts[word[:-1]] <= 1: # Se hace así por si la palabra restante tiene frecuencia 0 (no existe)\n",
        "                    test_tokens.append('<UNK>')\n",
        "                else:\n",
        "                    test_tokens.append(word[:-1])\n",
        "                test_tokens.append(word[-1])\n",
        "            # Si su frecuencia es 1 o menos, se agrega el token <UNK>\n",
        "            elif test_word_counts[word] <= 1:\n",
        "                test_tokens.append('<UNK>')\n",
        "            # Si su frecuencia es mayor a 1, se agrega la palabra\n",
        "            else:\n",
        "                test_tokens.append(word)\n",
        "    # Se agrega el token de fin de oracion\n",
        "    test_tokens.append('</s>')\n",
        "\n",
        "print(test_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se tokeniza el archivo 20N_GROUP_training.txt. Primero se encuentra la frecuencia de cada palabra para después reemplazar las palabras con frecuencia 1 por \\<UNK>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de oraciones en 20N_GROUP_training:  340220\n",
            "[('haywood', 1), ('sheffield-hallam', 1), ('uptodate', 1), ('termcap', 1), ('terminfo', 1), ('tilde', 1), ('per-user', 1), ('xsetup_', 1), ('(~user', 1), ('~user', 1)]\n"
          ]
        }
      ],
      "source": [
        "# Se cargan los archivos de training\n",
        "with open('20N_GROUP_training.txt', 'r', encoding='utf-8', errors='ignore') as file:\n",
        "    news_texts_train = file.read()\n",
        "\n",
        "# Vamos a separar el corpus de train de 20N en sentencias\n",
        "news_texts_train = news_texts_train.split('. ')\n",
        "print('Cantidad de oraciones en 20N_GROUP_training: ', len(news_texts_train))\n",
        "\n",
        "# Vamos a recorrer cada sentencia y separarla en palabras para agregarlas a un diccionario de frecuencias\n",
        "news_word_counts_train = Counter()\n",
        "for sentence in news_texts_train:\n",
        "    sentence = sentence.lower()\n",
        "    # Se separa la sentencia en palabras\n",
        "    sentence = sentence.split(' ')\n",
        "    # Si el ultimo caracter de una palabra no es una letra, se quita\n",
        "    for i in range(len(sentence)):\n",
        "        if sentence[i] != \"\" and not sentence[i][-1].isalpha():\n",
        "            sentence[i] = sentence[i][:-1]\n",
        "    # Se agregan las palabras al diccionario de frecuencias\n",
        "    news_word_counts_train.update(sentence)\n",
        "\n",
        "# Se revisan las palabras menos frecuentes\n",
        "print(news_word_counts_train.most_common()[:-10-1:-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora se procede a tokenizar el texto, estos se guardan en una lista."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de palabras en 20N_GROUP_training:  209342\n",
            "Cantidad de tokens en 20N_GROUP_training:  5261998\n"
          ]
        }
      ],
      "source": [
        "print('Cantidad de palabras en 20N_GROUP_training: ', len(news_word_counts_train))\n",
        "\n",
        "# Ahora se va a terminar de tokenizar el corpus de train de 20N\n",
        "news_tokens_train = []\n",
        "\n",
        "for sentence in news_texts_train:\n",
        "    # Primero se agrega el token de inicio de oracion\n",
        "    news_tokens_train.append('<s>')\n",
        "    # Se normaliza la sentencia\n",
        "    sentence = sentence.lower()\n",
        "    # Se separa la sentencia en palabras\n",
        "    sentence = sentence.split(' ')\n",
        "    # Se van revisando las palabras de la sentencia\n",
        "    for word in sentence:\n",
        "        # Si la palabra es un numero, se agrega el token <NUM>\n",
        "        if word.isnumeric():\n",
        "            news_tokens_train.append('<NUM>')\n",
        "        # Si la palabra no es vacia\n",
        "        elif word != \"\":\n",
        "            # Si la palabra termina en algo que no sea una letra o un numero\n",
        "            # se quita el ultimo caracter de la palabra y se agregan las dos partes\n",
        "            if not (word[-1].isalpha() or word[-1].isnumeric()):\n",
        "                # Si la palabra restante es un numero, se agrega el token <NUM>\n",
        "                if word[:-1].isnumeric():\n",
        "                    news_tokens_train.append('<NUM>')\n",
        "                # Si la palabra restante tiene frecuencia 1 o menos, se agrega el token <UNK>\n",
        "                elif news_word_counts_train[word[:-1]] <= 1: # Se hace así por si la palabra restante tiene frecuencia 0 (no existe)\n",
        "                    news_tokens_train.append('<UNK>')\n",
        "                else:\n",
        "                    news_tokens_train.append(word[:-1])\n",
        "                news_tokens_train.append(word[-1])\n",
        "            # Si su frecuencia es 1 o menos, se agrega el token <UNK>\n",
        "            elif news_word_counts_train[word] <= 1:\n",
        "                news_tokens_train.append('<UNK>')\n",
        "            # Si su frecuencia es mayor a 1, se agrega la palabra\n",
        "            else:\n",
        "                news_tokens_train.append(word)\n",
        "    # Se agrega el token de fin de oracion\n",
        "    news_tokens_train.append('</s>')\n",
        "\n",
        "print('Cantidad de tokens en 20N_GROUP_training: ', len(news_tokens_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<s>', 'the', 'm', 'code', 'stream', 'might', 'be', 'independently', 'attacked', 'based', 'on', 'knowledge', 'of', 'clipper', 'chip', 'protocols', 'as', 'revealed', 'plaintext', '</s>', '<s>', 'this', 'could', 'be', 'invalidated', 'by', 'changing', 'the', 'temporal', 'and', 'or', 'spatial', 'relationship', 'of', 'the', 'clipper', 'm', 'stream', 'and', 'the', 'actual', 'transmitted', 'stream', ',', 'under', 'the', 'control', 'of', 'a', 'secure', 'key', 'generator', 'synchronized', 'between', 'endpoints', '</s>', '<s>', 'the', 'useful', 'life', 'time', 'of', 'captured', 'law', 'enforcement', 'blocks', 'might', 'be', 'limited', 'based', 'on', 'hostile', 'forces', 'using', 'them', 'as', 'targets', 'following', 'transmission', 'interception', '</s>', '<s>', 'you', 'would', 'need', 'a', 'large', 'number', 'of', 'them', ',', 'but', ',', 'hey', \"there's\", 'supposed', 'to', 'be', 'millions', 'of', 'these', 'things', ',', 'right', '?', 'adding', 'time', 'stamps', 'to', 'the', 'encrypted', 'law', 'enforcement', 'block', 'is', 'probably', 'impractical', ',', 'who', 'wants', 'an', 'encryption', 'chip', 'with', 'a', 'real', 'time', 'clock', '?', '****************************************************************************', '*', 'the', 'entire', 'idea', 'of', 'the', 'law', 'enforcement', 'block', 'can', 'be', 'invalidated', '</s>', '<s>', 'i', 'just', 'had', 'the', 'thought', ',', 'that', 'you', 'could', 'capture', 'your', 'own', 'law', 'enforcement', 'blocks', 'for', 'session', 'keys', 'k', 'that', 'you', 'will', 'not', 'use', 'in', 'actual', 'transmissions', 'as', 'the', 'session', 'key', 'authenticators', '</s>', '<s>', 'the', 'proviso', 'that', 'you', \"don't\", 'mind', 'your', 'own', 'serial', 'number', 'being', 'discovered', '</s>', '<s>', 'd', '</s>', '<s>', 'denning', 'just', 'sent', 'out', 'further', 'information', 'of', 'a', 'new', 'version', 'of', 'the', 'clipper', 'chip', '</s>', '<s>', 'if', 'a', 'hash', 'function', 'were', 'to', 'be', 'embedded', 'in', 'a', 'clipper', 'm', 'transmission', 'block', 'reflecting', 'the', 'law', 'enforcement', 'block', ',', 'it', 'better', 'not', 'fall', 'on', '<NUM>', 'bit', 'block', 'boundaries', '</s>', '<s>', 'if', 'it', 'were', 'a', '<UNK>', 'datum', ',', 'you', 'could', 'lie', 'with', 'it', 'too', '</s>', '<s>', 'i', 'like', 'the', 'randomizer', 'inclusion', 'in', 'the', '<UNK>', '</s>', '<s>', 'i', 'remember', 'reading', 'that', 'intel', 'had', 'an', 'approved', 'random', 'noise', 'source', 'on', 'silicon', ',', 'hence', 'the', 'ability', 'to', 'put', 'it', 'inside', '</s>', '<s>', 'you', 'ever', 'think', 'that', 'mykotronx', 'sounds', 'like', 'one', 'of', 'those', 'made', 'up', 'names', 'of', 'companies', 'used', 'as', 'fronts', 'for', 'intelligence', 'organizations', '?', 'i', 'think', 'i', 'should', 'also', 'point', 'out', 'that', 'the', 'mystical', 'des', 'engines', 'are', 'known', 'plaintext', 'engines', '(unless', 'you', 'add', 'a', 'ton', 'of', 'really', 'smart', 'hardware?', ')', 'assume', 'the', 'ton', 'of', 'smart', 'hardware', '</s>', '<s>', 'it', \"doesn't\", 'really', 'have', 'to', 'be', 'that', 'smart', '</s>', '<s>', 'g', 'besides', 'being', 'an', 'infringement', 'on', 'our', 'civil', 'liberties', '(not', 'the', 'subject', 'in', 'this', 'post)', ',', 'the', 'name', '\"clipper', 'chip', '\"', 'seems', 'very', '<UNK>', 'with', 'the', '\"clipper', '\"', 'chip', 'of', 'intergraph', '</s>', '<s>', 'originally', 'designed', 'by', 'a', 'team', 'at', 'fairchild', 'semiconductor', ',', 'clipper', 'was', 'a', '32-bit', 'risc', 'microprocessor', '</s>', '<s>', 'it', 'is', 'still', 'used', 'in', 'some', 'workstations', ',', 'notably', 'those', 'from', 'intergraph', ',', 'the', 'supplier', 'of', 'cad', 'tools', '</s>', '<s>', 'intergraph', 'acquired', 'the', 'clipper', 'product', 'line', 'when', 'fairchild', 'was', 'sold', 'to', 'national', 'semiconductor', 'several', 'years', 'back', '</s>', '<s>', 'when', 'i', 'first', 'saw', '\"clipper', 'chip', '\"', 'in', 'the', 'announcement', ',', 'i', 'immediately', 'thought', 'the', 'article', 'was', 'referring', 'to', 'the', 'clipper', 'chip', 'i', 'know', '</s>', '<s>', 'this', 'seems', 'to', 'be', 'grounds', 'for', 'intergraph', 'to', 'sue', ',', 'but', 'then', \"i'm\", 'not', 'a', 'lawyer', '</s>', '<s>', \"i'd\", 'say', \"i'm\", 'a', 'cryptologist', ',', 'but', 'i', \"don't\", 'want', 'to', 'incriminate', 'myself', 'under', 'the', 'laws', 'of', 'the', 'new', 'regime', '</s>', '<s>', '-tim', 'may', '-', '-', '</s>', '<s>', 'timothy', 'c', '</s>', '<s>', 'may', '', '|', 'crypto', 'anarchy', ':', 'encryption', ',', 'digital', 'money', ',', 'tcmay@netcom', '</s>', '<s>', 'com', '', '|', 'anonymous', 'networks', ',', 'digital', 'pseudonyms', ',', 'zero', '<UNK>', '', '|', 'knowledge', ',', 'reputations', ',', 'information', 'markets', ',', 'w', '</s>', '<s>', 'a', '</s>', '<s>', 's', '</s>', '<s>', 't', '</s>', '<s>', 'e', '</s>', '<s>', '', ':', 'aptos', ',', 'ca', '', '|', 'black', 'markets', ',', 'collapse', 'of', 'governments', '</s>', '<s>', 'higher', 'power', ':', '<UNK>', '', '|', 'public', 'key', ':', 'pgp', 'and', 'mailsafe', 'available', '</s>', '<s>', 'whughes@lonestar', '</s>', '<s>', 'utsa', '</s>', '<s>', 'edu', '(william', 'w', '</s>', '<s>', 'hughes', ')', 'writes', ':', '>hell', ',', 'just', 'set', 'up', 'a', 'spark', 'jammer', ',', 'or', 'some', 'other', '_very', '_', 'electrically-noisy', '>device', '</s>', '<s>', 'as', \"i've\", 'noted', ',', 'you', 'can', 'likely', 'get', 'around', 'that', 'with', 'a', 'directional', 'sensor', '</s>', '<s>', 'phased', 'array', 'systems', 'could', 'completely', 'defeat', 'this', 'scheme', '</s>', '<s>', '-', '-', 'perry', 'metzger\\t\\tpmetzger@shearson', '</s>', '<s>', 'com', '-', '-', 'laissez', 'faire', ',', 'laissez', 'passer', '</s>', '<s>', 'le', 'monde', 'va', 'de', 'lui', 'meme', '</s>', '<s>', 'in', 'article', '<UNK>', '</s>', '<s>', '<UNK>', '</s>', '<s>', 'co', '</s>', '<s>', 'uk>', ',', 'graham', 'toal', '<gtoal@gtoal', '</s>', '<s>', 'com', '>', 'writes', ':', '', '>', 'in', 'article', '<naglec5w79e', '</s>', '<s>', '7hl@netcom', '</s>', '<s>', 'com', '>', 'nagle@netcom', '</s>', '<s>', 'com', '(john', 'nagle', ')', 'writes', ':', '', '>', '', ':', 'since', 'the', 'law', 'requires', 'that', 'wiretaps', 'be', 'requested', 'by', 'the', 'executive', '', '>', ':branch', 'and', 'approved', 'by', 'the', 'judicial', 'branch', ',', 'it', 'seems', 'clear', 'that', 'one', '', '>', ':of', 'the', 'key', 'registering', 'bodies', 'should', 'be', 'under', 'the', 'control', 'of', 'the', '', '>', ':judicial', 'branch', '</s>', '<s>', 'i', 'suggest', 'the', 'supreme', 'court', ',', 'or', ',', 'regionally', ',', 'the', '', '>', ':courts', 'of', 'appeal', '</s>', '<s>', 'more', 'specifically', ',', 'the', 'offices', 'of', 'their', 'clerks', '</s>', '<s>', '', '>', '', '>', \"i've\", 'got', 'a', 'better', 'idea', '</s>', '<s>', 'we', 'give', 'one', 'set', 'to', 'the', 'kgb', 'c/o', 'washington', 'embassy', ',', '', '>', 'and', 'the', 'other', 'set', 'to', 'the', 'red', 'chinese', '</s>', '<s>', '', '>', 'imho', ',', 'one', 'should', 'place', 'the', '<UNK>', 'into', 'satellites', '(space', ')', '</s>', '<s>', 'the', 'recovery', 'should', 'be', 'done', 'only', 'by', '(highly', 'visible', ')', 'teams', 'of', '<UNK>', '</s>', '<s>', '-', '-', 'borut', 'b', '</s>', '<s>', 'lavrencic', ',', 'd', '</s>', '<s>', 'sc', '</s>', '<s>', '', '|', 'x', '</s>', '<s>', '<NUM>', ':c=si;a=mail;p=ac;o=ijs;s=lavrencic', 'j', '</s>', '<s>', 'stefan', 'institute', '', '|', 'internet:borut', '</s>', '<s>', 'b', '</s>', '<s>', 'lavrencic@ijs', '</s>', '<s>', 'si', 'university', 'of', 'ljubljana', ',', '', '|', 'phone', ':', '+', '<NUM>', '<NUM>', '<NUM>', '<NUM>', '<UNK>', 'ljubljana', ',', 'slovenia', '|', '\\t', 'pgp', 'public', 'key', 'available', 'on', 'request', 'dolgo', 'smois', 'kalis', 'ovraz', 'nikei', 'njihk', 'ocnoo', 'dkril', 'ivseb', 'ipika', 'in', 'previous', 'postings', 'by', 'hellman', ',', 'bellovin', ',', '<UNK>', 'it', 'was', 'reported', 'the', 'big', 'brother', '(clipper', ')', 'chip', 'will', 'encrypt', 'plaintext', 'in', '<NUM>', 'bit', 'blocks', 'as', 'is', 'done', 'with', 'des', '</s>', '<s>', 'the', 'users', 'key', 'k', 'which', 'can', 'be', 'derived', 'from', 'rsa', ',', 'diffy-hellman', ',', 'etc', '</s>', '<s>', 'is', 'used', 'to', 'encrypt', 'plaintext', 'm', 'and', 'is', 'then', 'used', 'to', 'form']\n"
          ]
        }
      ],
      "source": [
        "print(news_tokens_train[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora se empieza obteniendo las frecuencias de cada palabra en el corpus de training. Se crea un diccionario que contiene la frecuencia de cada palabra."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de oraciones en BAC_GROUP_training:  7540764\n",
            "[('working/learning/reading', 1), ('shimrat', 1), ('irit', 1), ('theme!&nbsp', 1), ('maelene', 1), ('excitted', 1), ('chimms', 1), ('taurens', 1), ('arugh!!', 1), ('sb510', 1)]\n"
          ]
        }
      ],
      "source": [
        "with open('BAC_GROUP_training.txt', 'r', encoding='utf-8', errors='ignore') as file:\n",
        "    blogs_texts_train = file.read()\n",
        "\n",
        "# Vamos a separar el corpus de train de BAC en sentencias\n",
        "blogs_texts_train = blogs_texts_train.split('. ')\n",
        "print('Cantidad de oraciones en BAC_GROUP_training: ', len(blogs_texts_train))\n",
        "\n",
        "# Vamos a recorrer cada sentencia y separarla en palabras para agregarlas a un diccionario de frecuencias\n",
        "blogs_word_counts_train = Counter()\n",
        "for sentence in blogs_texts_train:\n",
        "    sentence = sentence.lower()\n",
        "    # Se separa la sentencia en palabras\n",
        "    sentence = sentence.split(' ')\n",
        "    # Si el ultimo caracter de una palabra no es una letra, se quita\n",
        "    for i in range(len(sentence)):\n",
        "        if sentence[i] != \"\" and not sentence[i][-1].isalpha():\n",
        "            sentence[i] = sentence[i][:-1]\n",
        "    # Se agregan las palabras al diccionario de frecuencias\n",
        "    blogs_word_counts_train.update(sentence)\n",
        "\n",
        "# Se revisan las palabras menos frecuentes\n",
        "print(blogs_word_counts_train.most_common()[:-10-1:-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora se tokeniza el texto. Se reemplazan los números por \\<NUM> y las palabras con frecuencia 1 por \\<UNK>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de palabras en BAC_GROUP_training:  1303444\n",
            "Cantidad de tokens en BAC_GROUP_training:  134129367\n"
          ]
        }
      ],
      "source": [
        "print('Cantidad de palabras en BAC_GROUP_training: ', len(blogs_word_counts_train))\n",
        "\n",
        "# Ahora se va a terminar de tokenizar el corpus de train de BAC\n",
        "blogs_tokens_train = []\n",
        "\n",
        "for sentence in blogs_texts_train:\n",
        "    # Primero se agrega el token de inicio de oracion\n",
        "    blogs_tokens_train.append('<s>')\n",
        "    # Se normaliza la sentencia\n",
        "    sentence = sentence.lower()\n",
        "    # Se separa la sentencia en palabras\n",
        "    sentence = sentence.split(' ')\n",
        "    # Se van revisando las palabras de la sentencia\n",
        "    for word in sentence:\n",
        "        # Si la palabra es un numero, se agrega el token <NUM>\n",
        "        if word.isnumeric():\n",
        "            blogs_tokens_train.append('<NUM>')\n",
        "        # Si la palabra no es vacia\n",
        "        elif word != \"\":\n",
        "            # Si la palabra termina en algo que no sea una letra o un numero\n",
        "            # se quita el ultimo caracter de la palabra y se agregan las dos partes\n",
        "            if not (word[-1].isalpha() or word[-1].isnumeric()):\n",
        "                # Si la palabra restante es un numero, se agrega el token <NUM>\n",
        "                if word[:-1].isnumeric():\n",
        "                    blogs_tokens_train.append('<NUM>')\n",
        "                # Si la palabra restante tiene frecuencia 1 o menos, se agrega el token <UNK>\n",
        "                elif blogs_word_counts_train[word[:-1]] <= 1: # Se hace así por si la palabra restante tiene frecuencia 0 (no existe)\n",
        "                    blogs_tokens_train.append('<UNK>')\n",
        "                else:\n",
        "                    blogs_tokens_train.append(word[:-1])\n",
        "                blogs_tokens_train.append(word[-1])\n",
        "            # Si su frecuencia es 1 o menos, se agrega el token <UNK>\n",
        "            elif blogs_word_counts_train[word] <= 1:\n",
        "                blogs_tokens_train.append('<UNK>')\n",
        "            # Si su frecuencia es mayor a 1, se agrega la palabra\n",
        "            else:\n",
        "                blogs_tokens_train.append(word)\n",
        "    # Se agrega el token de fin de oracion\n",
        "    blogs_tokens_train.append('</s>')\n",
        "\n",
        "print('Cantidad de tokens en BAC_GROUP_training: ', len(blogs_tokens_train))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por facilidad se guarda un archivo con estos tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardamos los tokens de train en un archivo .pickle\n",
        "import pickle\n",
        "\n",
        "with open('BAC_training_tokens.pickle', 'wb') as file:\n",
        "    for token in blogs_tokens_train:\n",
        "        pickle.dump(token, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<s>', \"i've\", 'been', 'selling', 'books', 'online', 'since', 'april', ',', 'and', 'am', 'painfully', 'aware', 'of', 'the', 'vast', 'ocean', 'of', 'book-knowledge', 'that', 'i', \"haven't\", 'yet', 'begun', 'to', 'cross', '</s>', '<s>', 'but', 'even', 'before', 'i', 'set', 'out', 'on', 'that', 'voyage', ',', \"there's\", 'a', 'smaller', 'body', 'of', 'retailing', 'commonsense', 'i', 'ought', 'to', 'master', '</s>', '<s>', 'alas', 'for', 'me', '</s>', '<s>', \"who'd\", 'have', 'guessed', 'that', \"there's\", 'a', 'person', 'out', 'there', 'who', 'uses', 'orange', 'highlighter', 'in', 'an', 'otherwise', 'lovely', 'urllink', 'heritage', 'press', 'edition', 'of', 'christopher', 'marlowe', '?', \"what'd\", 'they', 'think', 'it', 'was', ',', 'a', 'textbook', '?', 'lesson', '#1', ':', \"don't\", 'wait', 'until', \"you're\", 'ready', 'to', 'sell', 'the', 'merchandise', 'to', 'give', 'it', 'a', 'careful', 'inspection', ',', 'even', 'if', 'it', 'looks', 'great', 'on', 'the', 'outside', '</s>', '<s>', 'welcome', 'to', 'the', 'blog', 'of', 'urllink', '<UNK>', 'books', '</s>', '<s>', \"i'm\", 'launching', 'this', 'blog', 'as', 'a', 'way', 'to', 'track', 'my', 'learning', 'curve', 'in', 'the', 'bookselling', 'business', ';', 'i', 'hope', 'it', 'will', 'be', 'fun', ',', 'useful', ',', 'and', 'above', 'all', ',', '<UNK>', 'plan', 'to', 'record', 'my', 'failures', 'and', 'stupidities', 'as', 'well', 'as', 'my', 'successes', 'and', 'insights', '</s>', '<s>', \"we'll\", 'be', 'right', 'back', 'after', 'this', 'word', 'from', 'our', 'sponsor', '</s>', '<s>', '</s>', '<s>', 'before', 'the', '<UNK>', 'nincompoops', 'started', 'urllink', 'whining', 'about', 'google', '</s>', '<s>', 'choice', 'quote', ':', 'behind', \"google's\", 'complex', 'ranking', 'system', 'is', 'a', 'simple', 'idea', ':', 'each', 'link', 'to', 'a', 'page', 'should', 'be', 'considered', 'a', 'vote', ',', 'and', 'the', 'pages', 'with', 'the', 'most', 'votes', 'should', 'be', 'ranked', 'first', '</s>', '<s>', 'this', 'elegant', 'approach', 'uses', 'the', 'distributed', 'intelligence', 'of', 'web', 'users', 'to', 'determine', 'which', 'content', 'is', 'most', 'relevant', '</s>', '<s>', 'but', 'what', 'is', 'good', 'for', 'google', 'is', 'not', 'necessarily', 'good', 'for', 'the', 'rest', 'of', 'the', 'web', '</s>', '<s>', 'yeah', ',', 'god', 'forbid', 'that', 'the', 'distributed', 'intelligence', 'of', 'web', 'users', 'should', 'influence', 'web', 'site', 'popularity', '</s>', '<s>', 'i', 'demand', 'more', 'votes', 'for', 'the', 'little', 'web', 'site', '!', 'popular', 'web', 'sites', 'should', 'be', 'made', 'to', 'hand', 'over', 'some', 'of', 'their', '<UNK>', 'got', 'enough', 'for', 'all', 'of', 'us', '!', 'ooooh', ',', 'and', 'you', 'know', 'what', '?', '(cue', 'foreboding', 'fahrenheit', '9/11', 'music', ')', 'i', 'did', 'a', 'google', 'search', 'on', '\"dick', 'cheney', '\"', 'and', 'got', 'over', 'one', 'million', 'hits', '!!', '!', 'i', 'smell', 'a', 'cover-up', '</s>', '<s>', 'read', 'this', 'young', \"woman's\", 'essay', 'on', '', '\"', 'urllink', 'why', 'i', 'am', 'not', 'a', 'republican', '</s>', '<s>', '', '\"', 'then', 'read', 'the', 'rest', 'of', 'urllink', 'her', 'blog', '</s>', '<s>', 'then', 'tell', 'me', 'you', \"don't\", 'love', 'her', '</s>', '<s>', 'wow', '!', 'take', 'the', 'christian', 'science', 'monitor', 'urllink', 'quiz', 'and', 'find', 'out', '</s>', '<s>', 'my', 'results', 'make', 'me', 'out', 'to', 'be', 'a', '\"realist', '</s>', '<s>', '', '\"', 'realists', ':', 'are', 'guided', 'more', 'by', 'practical', 'considerations', 'than', 'ideological', 'vision', 'believe', 'us', 'power', 'is', 'crucial', 'to', 'successful', 'diplomacy', '', '-', 'and', 'vice', 'versa', \"don't\", 'want', 'us', 'policy', 'options', 'unduly', 'limited', 'by', 'world', 'opinion', 'or', 'ethical', 'considerations', 'believe', 'strong', 'alliances', 'are', 'important', 'to', 'us', 'interests', 'weigh', 'the', 'political', 'costs', 'of', 'foreign', 'action', 'believe', 'foreign', 'intervention', 'must', 'be', 'dictated', 'by', 'compelling', 'national', 'interest', 'historical', 'realist', ':', 'president', 'dwight', 'd', '</s>', '<s>', 'eisenhower', 'modern', 'realist', ':', 'secretary', 'of', 'state', 'colin', 'powell', 'so', 'i', 'wasted', 'a', 'whole', 'saturday', 'futzing', 'around', 'on', 'the', 'computer', ';', \"what's\", 'your', 'point', '?', 'anyway', ',', 'i', 'just', 'bought', 'an', 'entry-level', 'digital', 'camera', 'to', 'take', 'pictures', 'of', 'sets', 'of', 'books', '(too', 'hard', 'to', 'get', 'a', 'dozen', 'of', 'them', 'to', 'sit', 'still', 'on', 'the', 'scanner', ')', '</s>', '<s>', 'thought', \"i'd\", 'try', 'it', 'out', 'on', 'daisy', ',', 'our', 'five-year-old', 'housemate', ':', 'the', 'cats', 'have', 'taught', 'her', 'to', 'affect', 'an', 'air', 'of', 'indifference', ',', 'but', 'i', 'can', 'tell', \"she's\", 'secretly', 'impressed', '</s>', '<s>', 'taking', 'a', 'page', 'from', 'urllink', 'keith', '<UNK>', '', ',', \"i'm\", 'going', 'to', 'post', 'quotes', 'from', 'academic', 'journal', 'articles', 'and', 'books', 'that', 'strike', 'me', 'as', 'absurd', ',', 'outrageous', ',', 'reality-challenged', ',', 'or', 'some', 'tasty', 'combination', 'thereof', '</s>', '<s>', 'as', 'i', 'lack', 'his', 'philosophical', 'charity', ',', 'however', ',', 'i', \"won't\", 'be', 'able', 'to', 'refrain', 'from', 'appending', 'snarky', 'comments', 'to', 'them', '</s>', '<s>', \"here's\", 'the', 'inaugural', '<UNK>', ':', 'the', 'already', 'ambiguous', 'conflation', 'of', 'nation-state', '(soviet', ')', 'and', 'ideology', '(communist', ')', 'in', 'the', 'portrayal', 'of', 'enemies', 'abstracted', 'and', 'covered', 'over', 'a', '<UNK>', 'set', 'of', 'repressions', 'that', 'not', 'only', 'silenced', 'women', ',', 'but', 'also', 'others', 'colonized', 'by', 'national', 'projects', ',', 'whose', 'objections', 'or', 'affiliations', 'might', 'bring', 'them', 'fatally', 'back', 'into', 'view', '</s>', '<s>', 'the', 'subject', 'of', 'the', 'cold', 'war', 'was', 'military', ',', 'although', 'the', 'war', 'enlisted', 'all', 'of', 'everyday', 'life', '</s>', '<s>', 'this', 'had', 'the', 'effect', 'of', 'making', 'all', 'other', 'discourses', 'seem', 'not', 'only', 'internal', ',', 'but', 'also', 'more', 'subjective', ',', 'more', 'unreal', 'as', 'more', 'distant', 'from', 'the', 'military', 'project', '</s>', '<s>', 'even', 'the', 'struggle', 'between', 'science', 'and', 'the', 'national', 'interest', 'was', 'not', 'won', 'by', 'science', ',', 'and', 'those', 'scientists', 'who', 'insisted', 'on', 'sharing', 'atomic', 'secrets', 'could', 'be', 'put', 'to', 'death', 'as', 'spies', '</s>', '<s>', 'maximum', 'objectivity', 'and', 'realism', 'was', 'not', 'attributed', 'to', 'the', 'scientists', 'who', 'advocated', 'open', 'international', 'inquiry', ',', 'but', 'rather', 'to', 'the', 'bureaucracy', 'that', 'took', 'away', 'their', 'passports', '</s>', '<s>', '[snip', 'of', 'some', 'previews', 'of', 'chapter', '<NUM>', ']', 'surely', 'the', 'rosenberg', 'executions', 'enforced', 'not', 'just', 'power', 'but', 'belief', 'in', 'the', 'possibility', 'of', 'subversion', 'through', 'sharing', 'of', 'scientific', 'knowledge', ',', 'now', 'conflated', 'with', 'the', 'development', 'of', 'weaponry', '</s>', '<s>', 'the', 'stakes', 'were', 'the', 'highest', ':', 'the', 'arms', 'race', 'cost', 'more', 'than', 'both', 'world', 'wars', 'combined', '</s>', '<s>', '<UNK>', '\"', 'became', 'a', 'standard', 'of', 'proof', 'and', 'mark', 'of', 'suspicion', 'shared', 'by', 'nuclear', 'weapons', 'negotiations', 'and', 'the', 'philosophy', 'of', 'science', '(9', ')', '</s>', '<s>', '<UNK>', ',', 'suzanne', '</s>', '<s>', 'cold', 'warriors', ':', 'manliness', 'on', 'trial', 'in', 'the', 'rhetoric', 'of', 'the', 'west', '</s>', '<s>', 'carbondale', ':', 'southern', 'illinois', 'up', ',', '<NUM>', '</s>', '<s>', 'i', '<UNK>', '-', 'grieve', '--over', 'the', 'cruel', 'fate', 'suffered', 'by', 'those', 'starry-eyed', 'scientists', 'whose', 'passports', 'were', 'revoked', 'by', 'a', 'hypermasculine', ',', '<UNK>', ',', '<UNK>', 'idolizing', ',', 'war-mongering', '\"united', '\"', '\"states', '\"', '\"culture', '</s>', '<s>', '', '\"', 'the', 'world', 'would', 'have', 'been', 'much', 'better', 'off', 'had', 'their', 'attempts', 'to', 'sustain', '\"open', 'inquiry', '\"', 'succeeded', '</s>', '<s>', 'and', 'the', 'brave', 'rosenbergs', 'were', 'just', 'making', 'a', 'desperate', 'bid', 'for', 'peace', ',', 'or', 'at', 'least', 'fiscal', 'prudence', '(seeing', 'as', 'how', 'the', 'arms', 'race', 'was', 'so', 'much', 'more', 'expensive', 'than', 'both', 'world', 'wars', 'combined', ')', '</s>', '<s>', 'how', 'i', 'wish', \"we'd\", 'listened', 'to', 'the', '<UNK>', ';', 'had', 'the', 'soviets', 'prevailed', ',', 'the', 'national', 'project', 'colonizing', 'us', 'would', 'have', 'been', 'complete', 'and', 'total', ',', 'and', 'therefore', 'much', 'more', 'egalitarian', 'than', 'the', 'selective', 'colonization', 'of', 'the', 'tragically', 'unhip', 'cold', 'war', 'era', '</s>', '<s>', 'one', 'of', 'my', 'favorite', 'blogs', 'urllink', 'passes', 'the', 'one', 'year', 'mark', 'today', '</s>', '<s>', 'alex', 'tabarrok', 'and', 'tyler', 'cowen', 'have', 'earned', 'the', 'right', 'to', 'crow', ';', 'readers', 'are', 'guaranteed', 'to', 'always', 'learn', 'something', 'fascinating', 'from', 'them', '</s>', '<s>', 'the', 'winter', '2003-2004', 'issue', 'of', 'academic', 'questions', '', ',', 'the', 'journal', 'of', 'the', 'urllink', 'national', 'association', 'of', 'scholars', '--check', 'them', 'out', 'if', 'you', \"don't\", 'know', 'who', 'they', '<UNK>', 'in', 'my', 'mailbox', 'yesterday', '(so', \"it's\", 'a', 'little', 'late', ')', '</s>', '<s>', 'in', 'it', ',', 'urllink', 'mark', 'bauerlein', 'writes', 'a', 'tough', 'review', 'of', 'gerald', \"graff's\", 'book', ',', 'urllink', 'clueless', 'in', 'academe', ':', 'how', 'schooling', 'obscures', 'the', 'life', 'of', 'the', 'mind', '</s>', '<s>', 'graff', 'is', 'probably', 'best', 'known', 'for', 'his', 'book', 'urllink', 'teaching', 'the', 'conflicts', '', ',', 'which', 'argues', 'that', 'poststructuralist', '\"disciplinary', '\"', 'hypertrophy', \"doesn't\", 'have', 'to', 'leave', 'students', 'behind', '</s>', '<s>', 'as', 'an', 'earnest', 'graduate', 'student', 'looking', 'for', 'guidance', 'in', 'teaching', ',', 'i', 'found', \"graff's\", 'approach', 'lucid', 'but', 'strangely', 'tepid', 'and', 'unsatisfying', ',', 'although', 'i', \"didn't\", 'really', 'analyze', 'why', '</s>', '<s>', 'bauerlein', ',', 'on', 'the', 'other', 'hand', ',', 'sticks', 'his', 'scalpel', 'right', 'into', 'the', 'bad', 'spots', '</s>', '<s>', 'i', 'wish', 'i', 'could', 'link', 'to', 'an', 'online', 'version', 'of', 'the', 'review', ',', 'or', 'scan', 'it', 'in', 'for', 'you', ',', 'but', 'that', 'would', 'be', 'wrong', ',', 'so', \"i'll\", 'just', 'quote', 'some', 'choice', 'bits', ':', '<UNK>', ']', 'name', 'echoes', 'of', 'past', 'controversies', 'and', 'his', 'book', 'sports', 'an', '<UNK>', 'title', ',', 'but', 'whether', 'a', 'noted', 'professor', 'in', 'his', 'twilight', 'can', 'muster', 'the', 'courage', 'and', '<UNK>', 'to', 'carry', 'out', 'a', 'thorough', 'professional', 'soul-searching', 'is', 'doubtful', '</s>', '<s>', 'graff', 'answers', 'the', 'question', 'in', 'his', 'very', 'first', 'sentence', ':', 'this', 'book', 'is', 'an', 'attempt', 'by', 'an', 'academic', 'to', 'look', 'at', 'academia', 'from', 'the', 'perspective', 'of', 'those', 'who', \"don't\", 'get', 'it', '</s>', '<s>', 'it', 'sounds', 'good', 'until', 'you', 'reach', 'the', 'last', 'three', 'words', '</s>', '<s>', 'to', 'see', 'academe', 'from', 'outside', 'the', 'campus', 'walls', 'and', 'faculty', 'cliques', 'is', 'a', '<UNK>', 'aim', '</s>', '<s>', 'if', 'humanities', 'professors', 'made', 'the', 'least', 'effort', 'to', 'understand', 'why', 'eugene', 'goodheart', ',', 'frederick', 'crews', ',', 'and', 'others', 'criticize', 'the', 'field', ',', 'instead', 'of', 'dismissing', 'them', 'as', 'reactionaries', 'and', 'dumbbells', ',', 'they', 'might', 'strengthen', 'their', 'own', 'positions', 'or', 'even', 'find', 'points', 'of', 'agreement', '</s>', '<s>', 'but', 'the', '\"don\\'t', 'get', 'it', '\"', 'phrase', 'shortchanges', 'the', 'outsiders', '</s>', '<s>', 'even', 'though', 'it', 'suggests', 'some', 'sympathy', 'for', 'them', ',', 'they', 'are', 'still', 'the', 'benighted', ',', 'the', 'confused', ',', 'people', 'who', 'feel', '\"shame', 'and', 'resentment', '\"', 'when', 'facing', 'the', '<UNK>', 'of', 'the', 'academic', 'world', '\"', '(91', ')', '</s>', '<s>', '', '[', '</s>', '<s>', '</s>', '<s>', '</s>', '<s>', '', ']', 'but', 'look', 'closely', 'at', 'at', \"graff's\", 'diagnosis', 'of', 'the', 'problems', 'and', 'you', 'find', 'an', 'evasion', 'and', 'a', 'rationalization', 'wholly', 'consistent', 'with', 'the', 'dithering', 'appraisals', 'of', 'his', 'generation', '</s>', '<s>', 'the', 'runaround', 'is', 'simple', ':', 'graff', 'attributes', 'the', 'breakdown', 'of', 'humanities', 'education', 'solely', 'to', 'a', 'rhetorical', 'failure', '</s>', '<s>', 'scholars', 'and', 'teachers', 'think', 'sharply', 'and', 'reason', 'skillfully', ',', 'he', 'insists', ',', 'but', 'they', \"don't\", 'express', 'their', 'ideas', 'in', 'limpid', 'speech', '</s>', '<s>', 'advances', 'in', 'curriculum', 'proceed', ',', 'breakthroughs', 'of', 'theory', 'have', 'transpired', ',', 'but', 'the', 'academic', 'idiom', \"hasn't\", 'articulated', 'them', 'well', '</s>', '<s>', 'students', 'shy', 'away', 'only', 'because', 'they', \"don't\", 'speak', 'the', 'professors', \"'\", 'language', '</s>', '<s>', 'in', 'class', ',', '\"once', 'students', 'have', 'to', 'translate', 'their', 'personal', 'interests', 'and', 'experience', 'into', 'the', 'formalized', 'conventions', 'of', 'written', 'arguespeak', ',', 'their', 'interests', 'and', 'experience', 'no', 'longer', 'seem', 'their', 'own', '</s>', '<s>', '', '\"', 'nothing', 'in', 'the', 'values', ',', 'principles', ',', 'and', 'knowledge', 'of', 'the', 'professors', 'is', 'askew', '</s>', '<s>', 'it', 'is', ',', 'rather', ',', 'only', 'the', 'communication', 'that', 'needs', 'fixing', '</s>', '<s>', 'the', 'humanities', 'are', 'dying', ',', 'but', 'graff', 'goes', 'no', 'further', 'than', 'urging', ',', '\"we', 'have', 'to', 'improve', 'our', 'message', '\"', '(93', ')', '</s>', '<s>', '', '[', '</s>', '<s>', '</s>', '<s>', '</s>', '<s>', '', ']', 'to', 'consider', 'an', '<UNK>', 'with', 'arguespeak', 'the', 'prime', 'shortcoming', 'of', 'students', 'is', 'to', 'overlook', 'other', ',', 'gaping', 'deficiencies', 'of', 'skill', 'and', 'knowledge', '</s>', '<s>', 'graff', 'appears', 'unconcerned', 'with', 'the', 'fact', 'that', 'high', 'school', 'graduates', \"can't\", 'write', 'a', 'periodic', 'sentence', ',', 'barely', 'understand', 'a', 'passage', 'of', 'prose', ',', 'disregard', 'the', 'classics', ',', 'and', \"can't\", 'pinpoint', 'the', 'half-century', 'in', 'which', 'the', 'civil', 'war', 'took', 'place', '</s>', '<s>', 'feeble', 'historical', 'learning', ',', 'declining', 'reading', 'scores', ',', 'pitiful', 'writing', '<UNK>', 'give', 'place', 'to', 'a', 'particular', 'forensic', ',', 'arguespeak', '</s>', '<s>', 'humanities', 'education', 'is', 'training', 'in', 'academic', 'discourse', ',', 'not', 'the', 'study', 'of', 'history', 'and', 'literature', '</s>', '<s>', 'rather', 'than', 'forming', 'students', 'into', 'learned', ',', 'eloquent', 'minds', ',', \"graff's\", 'pedagogy', 'shapes', 'them', 'into', 'canny', '<UNK>', ',', 'that', 'is', ',', 'into', 'junior', 'imitations', 'of', 'their', 'professors', '(94', ')', '</s>', '<s>', 'as', 'if', 'this', \"weren't\", 'enough', ',', '<UNK>', 'introductory', 'paragraphs', 'render', 'a', 'devastating', 'picture', 'of', 'the', 'rise', 'of', 'poststructuralist', 'scholars', ':', '<UNK>', 'scholars', 'in', 'the', '60s', 'and', '70s', ']', 'like', 'to', 'remember', 'those', 'years', 'as', 'an', 'uphill', 'struggle', 'against', 'old-fashioned', 'formalists', ',', 'biographers', ',', '<UNK>', ',', 'and', 'arts', '<UNK>', ',', 'but', 'i', 'have', 'yet', 'to', 'hear', 'of', 'a', 'hotshot', 'iconoclast', 'skilled', 'in', 'structuralism', 'who', 'suffered', 'for', 'his', 'beliefs', '</s>', '<s>', 'the', 'fact', 'is', ',', 'campus', 'conditions', 'favored', 'them', '</s>', '<s>', 'as', 'administrators', 'caved', 'in', 'to', 'various', 'protesters', ',', 'they', 'found', 'a', 'rationalization', 'in', 'the', '<UNK>', 'tailwind', 'and', 'jumped', 'to', 'invest', 'in', 'the', 'bold', 'new', 'critical', 'world', '</s>', '<s>', '', '[', '</s>', '<s>', '</s>', '<s>', '</s>', '<s>', '', ']', 'life', 'was', 'good', ',', 'and', 'one', 'can', 'hardly', 'blame', 'the', 'beneficiaries', 'if', 'they', 'attributed', 'the', 'largesse', 'to', 'their', 'own', 'abilities', 'instead', 'of', 'to', 'post-war', 'demographics', 'and', 'social', 'movements', '</s>', '<s>', 'why', 'should', 'a', 'cutting-edge', 'specialist', 'in', 'french', 'thought', 'ask', 'about', 'the', 'social', 'value', 'of', 'literary', 'theory', 'when', 'so', 'many', 'were', 'demanding', 'his', 'attention', '?', 'a', 'marxist', 'professor', 'invited', 'to', 'lecture', 'from', 'sydney', 'to']\n"
          ]
        }
      ],
      "source": [
        "print(blogs_tokens_train[:2000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **IV. Crear modelos de lenguaje**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.util import ngrams\n",
        "from nltk.probability import FreqDist, LaplaceProbDist\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modelo de lenguaje de N-gramas - 20N"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probabilidad de \"the\": 1.827705827091718e-07\n",
            "Probabilidad más alta: ('<s>',)\n"
          ]
        }
      ],
      "source": [
        "# Primero se crea el modelo de unigramas\n",
        "news_unigrams_train = ngrams(news_tokens_train, 1)\n",
        "\n",
        "# Se calculan las frecuencias de cada unigrama\n",
        "news_unigrams_freq = FreqDist(news_unigrams_train)\n",
        "\n",
        "# Se aplica suavizado laplaciano\n",
        "news_unigrams_prob = LaplaceProbDist(news_unigrams_freq, bins=len(news_word_counts_train))\n",
        "\n",
        "# Ejemplo de probabilidad de un unigrama\n",
        "probabilidad = news_unigrams_prob.prob('I')\n",
        "print(f'Probabilidad de \"the\": {probabilidad}')\n",
        "\n",
        "print(\"Probabilidad más alta:\",news_unigrams_prob.max())\n",
        "\n",
        "# Guardar el modelo de unigramas\n",
        "with open('20N_unigrams.pkl', 'wb') as file:\n",
        "    pickle.dump(news_unigrams_prob, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probabilidad de \"i am\": 0.0005458112506051984\n",
            "Probabilidad más alta: ('</s>', '<s>')\n"
          ]
        }
      ],
      "source": [
        "# Crear el modelo de bigramas\n",
        "news_bigrams_train = ngrams(news_tokens_train, 2)\n",
        "\n",
        "# Se calculan las frecuencias de cada bigrama\n",
        "news_bigrams_freq = FreqDist(news_bigrams_train)\n",
        "\n",
        "# Se aplica suavizado laplaciano\n",
        "news_bigrams_prob = LaplaceProbDist(news_bigrams_freq)\n",
        "\n",
        "# Ejemplo de probabilidad de un bigrama\n",
        "probabilidad = news_bigrams_prob.prob(('i', 'am'))\n",
        "print(f'Probabilidad de \"i am\": {probabilidad}')\n",
        "\n",
        "print(\"Probabilidad más alta:\", news_bigrams_prob.max())\n",
        "\n",
        "# Guardar el modelo de bigramas\n",
        "with open('20N_bigrams.pkl', 'wb') as file:\n",
        "    pickle.dump(news_bigrams_prob, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probabilidad de \"i am a\": 1.7704921726809303e-05\n",
            "Probabilidad más alta: ('<UNK>', '</s>', '<s>')\n"
          ]
        }
      ],
      "source": [
        "# Crear el modelo de trigramas\n",
        "news_trigrams_train = ngrams(news_tokens_train, 3)\n",
        "\n",
        "# Se calculan las frecuencias de cada trigramas\n",
        "news_trigrams_freq = FreqDist(news_trigrams_train)\n",
        "\n",
        "# Se aplica suavizado laplaciano\n",
        "news_trigrams_prob = LaplaceProbDist(news_trigrams_freq)\n",
        "\n",
        "# Ejemplo de probabilidad de un trigramas\n",
        "probabilidad = news_trigrams_prob.prob(('i', 'am', 'a'))\n",
        "print(f'Probabilidad de \"i am a\": {probabilidad}')\n",
        "\n",
        "print(\"Probabilidad más alta:\", news_trigrams_prob.max())\n",
        "\n",
        "# Guardar el modelo de trigramas\n",
        "with open('20N_trigrams.pkl', 'wb') as file:\n",
        "    pickle.dump(news_trigrams_prob, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modelo de lenguaje de N-gramas - BAC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar el archivo de tokens de BAC\n",
        "import pickle\n",
        "\n",
        "with open('BAC_training_tokens.pickle', 'rb') as file:\n",
        "    blogs_tokens_train = []\n",
        "    while True:\n",
        "        try:\n",
        "            blogs_tokens_train.append(pickle.load(file))\n",
        "        except EOFError:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probabilidad de \"the\": 7.429830470660517e-09\n",
            "probabilidad más alta: ('<s>',)\n"
          ]
        }
      ],
      "source": [
        "# Primero se crea el modelo de unigramas\n",
        "blogs_unigrams_train = ngrams(blogs_tokens_train, 1)\n",
        "\n",
        "# Se calculan las frecuencias de cada unigrama\n",
        "blogs_unigrams_freq = FreqDist(blogs_unigrams_train)\n",
        "\n",
        "# Se aplica suavizado laplaciano\n",
        "blogs_unigrams_prob = LaplaceProbDist(blogs_unigrams_freq)\n",
        "\n",
        "# Ejemplo de probabilidad de un unigrama\n",
        "probabilidad = blogs_unigrams_prob.prob('i')\n",
        "print(f'Probabilidad de \"the\": {probabilidad}')\n",
        "\n",
        "print(\"probabilidad más alta:\", blogs_unigrams_prob.max())\n",
        "\n",
        "# Guardar el modelo de unigramas\n",
        "with open('BAC_unigrams.pkl', 'wb') as file:\n",
        "    pickle.dump(blogs_unigrams_prob, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probabilidad de \"i am\": 0.001330170538731935\n",
            "probabilidad más alta: ('</s>', '<s>')\n"
          ]
        }
      ],
      "source": [
        "# Crear el modelo de bigramas\n",
        "blogs_bigrams_train = ngrams(blogs_tokens_train, 2)\n",
        "\n",
        "# Se calculan las frecuencias de cada bigrama\n",
        "blogs_bigrams_freq = FreqDist(blogs_bigrams_train)\n",
        "\n",
        "# Se aplica suavizado laplaciano\n",
        "blogs_bigrams_prob = LaplaceProbDist(blogs_bigrams_freq)\n",
        "\n",
        "# Ejemplo de probabilidad de un bigrama\n",
        "probabilidad = blogs_bigrams_prob.prob(('i', 'am'))\n",
        "\n",
        "print(f'Probabilidad de \"i am\": {probabilidad}')\n",
        "\n",
        "print(\"probabilidad más alta:\", blogs_bigrams_prob.max())\n",
        "\n",
        "# Guardar el modelo de bigramas\n",
        "with open('BAC_bigrams.pkl', 'wb') as file:\n",
        "    pickle.dump(blogs_bigrams_prob, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el modelo de bigramas\n",
        "blogs_bigrams_train = ngrams(blogs_tokens_train, 2)\n",
        "\n",
        "# Se calculan las frecuencias de cada bigrama\n",
        "blogs_bigrams_freq = FreqDist(blogs_bigrams_train)\n",
        "\n",
        "# Se aplica suavizado laplaciano\n",
        "blogs_bigrams_prob = LaplaceProbDist(blogs_bigrams_freq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El modelo de trigramas no corrió :\\( Se intentaron muchas formas, siendo la última la siguiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Crear el modelo de trigramas\n",
        "blogs_trigrams_train = ngrams(blogs_tokens_train, 3)\n",
        "\n",
        "# Se calculan las frecuencias de cada trigrama iterativamente para ahorrar memoria\n",
        "freq_dist = {}\n",
        "for trigram in blogs_trigrams_train:\n",
        "    freq_dist[trigram] = freq_dist.get(trigram, 0) + 1\n",
        "\n",
        "# Se aplica suavizado laplaciano iterativamente para ahorrar memoria y se guarda en un archivo .txt dinamicamente\n",
        "with open('BAC_trigrams.txt', 'w') as file:\n",
        "    trigram_prob = {}\n",
        "    for trigram in blogs_trigrams_train:\n",
        "        trigram_prob[trigram] = (freq_dist.get(trigram, 0) + 1) / (blogs_bigrams_freq[(trigram[0], trigram[1])] + len(freq_dist.keys()))\n",
        "        file.write(str(trigram) + ' ' + str(trigram_prob[trigram]) + '\\n')\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
